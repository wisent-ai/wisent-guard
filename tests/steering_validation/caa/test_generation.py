#!/usr/bin/env python3
"""
Test CAA text generation consistency by comparing our implementation
with reference data generated by the original CAA implementation.

This test validates that our wisent-guard CAA implementation produces
identical text generation when given the same:
- Model (meta-llama/Llama-2-7b-hf)
- Prompts
- Generation settings (greedy decoding, same seed)
- Steering vector (for steered test)

Tests both unsteered and steered generation paths.
"""

import json
import torch
import pytest
from pathlib import Path
import sys

# Add wisent-guard to path
WISENT_PATH = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(WISENT_PATH))

from wisent_guard.core.steering_methods.caa import CAA

from .const import (
    MODEL_NAME,
    LAYER_INDEX,
    DEVICE,
    MAX_NEW_TOKENS,
    TOP_K,
    RANDOM_SEED,
    TEXT_COMPLETIONS_UNSTEERED_PATH,
    TEXT_COMPLETIONS_STEERED_PATH,
    HALLUCINATION_VECTOR_PATH,
)
from .model_utils import RealModelWrapper
from .caa_utils import tokenize_llama_base_format
from ..utils import aggressive_memory_cleanup


def load_reference_text_completions(steered=False):
    """Load pre-generated reference text completions."""
    if steered:
        ref_path = TEXT_COMPLETIONS_STEERED_PATH
        completion_type = "steered"
    else:
        ref_path = TEXT_COMPLETIONS_UNSTEERED_PATH
        completion_type = "unsteered"

    print(f"ðŸ”¬ Loading reference {completion_type} text completions...")

    if not ref_path.exists():
        pytest.skip(f"Reference {completion_type} text completions not found at {ref_path}")

    with open(ref_path, "r") as f:
        reference_results = json.load(f)

    print(f"âœ… Loaded {len(reference_results)} reference {completion_type} text completions")
    return reference_results


def generate_with_wisent_caa(prompts, steering_vector=None, strength=1.0):
    """Generate text using our wisent-guard implementation with exact CAA parameters."""
    is_steered = steering_vector is not None
    print(f"ðŸ¦™ Generating with Wisent-Guard ({'steered' if is_steered else 'unsteered'})...")

    # Initialize model wrapper
    model = RealModelWrapper(MODEL_NAME, device=DEVICE)

    try:
        results = []

        # Prepare steering if needed
        steering_hook_handle = None
        if is_steered:
            caa = CAA(device=DEVICE, legacy_behavior=True)  # Use legacy behavior to match original CAA
            caa.steering_vector = steering_vector.to(DEVICE)
            caa.layer_index = LAYER_INDEX
            caa.is_trained = True

            # Create steering hook
            def steering_hook(module, input, output):
                if isinstance(output, tuple) and len(output) > 0:
                    # Apply steering to the hidden states
                    steered = caa.apply_steering(output[0], strength=strength)
                    output = (steered,) + output[1:]
                return output

            # Register hook on the appropriate layer
            layer = model.model.model.layers[LAYER_INDEX]
            steering_hook_handle = layer.register_forward_hook(steering_hook)

        for i, prompt_data in enumerate(prompts):
            user_input = prompt_data["question"]
            print(f"  Processing prompt {i + 1}/{len(prompts)}: {user_input[:50]}...")

            # Use exact same tokenization as reference
            prompt_tokens = tokenize_llama_base_format(model.tokenizer, user_input)
            tokens = torch.tensor(prompt_tokens).unsqueeze(0).to(DEVICE)

            # Set random seed for reproducibility (same as reference)
            torch.manual_seed(RANDOM_SEED)

            # Generate text with exact same parameters as reference
            with torch.no_grad():
                generated_ids = model.model.generate(
                    inputs=tokens,
                    max_new_tokens=MAX_NEW_TOKENS,  # 50
                    top_k=TOP_K,  # 1 (greedy decoding)
                    pad_token_id=model.tokenizer.eos_token_id,
                )

                # Decode generated text (same as reference)
                generated_text = model.tokenizer.batch_decode(generated_ids)[0]

            # Calculate prompt and response parts (same format as reference)
            prompt_str = f"Input: {user_input}\nResponse:"
            response_part = (
                generated_text[len(prompt_str) :].strip() if prompt_str in generated_text else generated_text.strip()
            )

            results.append(
                {
                    "prompt": prompt_str,
                    "generated_full": generated_text,
                    "generated_response": response_part,
                    "tokens": generated_ids[0].tolist(),
                    "question": user_input,
                    "steered": is_steered,
                }
            )

            print(f"    Generated: {response_part[:100]}...")

    finally:
        # Clean up hook
        if steering_hook_handle is not None:
            steering_hook_handle.remove()

        # Clean up model to free GPU memory
        del model
        aggressive_memory_cleanup()

    print(f"âœ… Wisent-Guard {'steered' if is_steered else 'unsteered'} generation complete")
    return results


@pytest.mark.slow
class TestCAAGeneration:
    """Test suite for CAA text generation validation."""

    def test_unsteered_generation(self):
        """Test that our unsteered generation matches reference exactly."""
        print("\nðŸŽ¯ Testing unsteered generation consistency...")

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        # Check available GPU memory
        if torch.cuda.is_available():
            memory_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
            memory_free_gb = memory_free / (1024**3)
            print(f"Available GPU memory: {memory_free_gb:.2f} GB")

            if memory_free_gb < 15.0:
                pytest.skip(f"Insufficient GPU memory: {memory_free_gb:.2f} GB available, need 15+ GB")

        # Load reference unsteered data
        reference_results = load_reference_text_completions(steered=False)

        # Extract prompts for generation
        test_prompts = []
        for ref in reference_results:
            # Extract question from the reference data
            if "question" in ref:
                test_prompts.append({"question": ref["question"]})
            else:
                # Parse from prompt if question field not available
                prompt_text = ref["prompt"]
                if "Input: " in prompt_text:
                    question = prompt_text.split("Input: ")[1].split("\nResponse:")[0].strip()
                    test_prompts.append({"question": question})

        print(f"Loaded {len(test_prompts)} test prompts for unsteered generation")

        # Generate with our implementation (no steering)
        our_results = generate_with_wisent_caa(test_prompts, steering_vector=None)

        # Compare results
        print(f"\nðŸ“Š Comparing unsteered generation results...")

        total_prompts = min(len(our_results), len(reference_results))
        high_match_count = 0

        for i in range(total_prompts):
            our_result = our_results[i]
            ref_result = reference_results[i]

            prompt = our_result["prompt"][:50] + "..."

            print(f"\n  Prompt {i + 1}: {prompt}")
            print(f"    Our:       {our_result['generated_response'][:80]}...")
            print(f"    Reference: {ref_result['generated_response'][:80]}...")

            # Compare token sequences
            our_tokens = our_result["tokens"]
            ref_tokens = ref_result["tokens"]

            # Calculate match rate
            min_token_len = min(len(our_tokens), len(ref_tokens))
            matching_tokens = sum(1 for j in range(min_token_len) if our_tokens[j] == ref_tokens[j])
            token_match_rate = matching_tokens / min_token_len if min_token_len > 0 else 0.0

            print(f"    Token match rate: {matching_tokens}/{min_token_len} ({token_match_rate:.2%})")

            # Categorize match quality
            if token_match_rate >= 0.9:
                print("    âœ… High consistency achieved")
                high_match_count += 1
            elif token_match_rate >= 0.7:
                print("    âš ï¸  Moderate consistency")
            else:
                print("    âŒ Low consistency")
                # Show first few differing tokens for debugging
                for j in range(min(5, min_token_len)):
                    if our_tokens[j] != ref_tokens[j]:
                        print(f"      Token {j}: {our_tokens[j]} vs {ref_tokens[j]}")

            # Basic assertion for unsteered - should be very high match rate
            assert token_match_rate > 0.5, f"Very low token match rate for unsteered generation: {token_match_rate:.2%}"

        # Overall assessment
        overall_quality = high_match_count / total_prompts
        print(
            f"\nðŸ“ˆ Overall unsteered generation quality: {high_match_count}/{total_prompts} ({overall_quality:.2%}) high matches"
        )

        # Assert reasonable overall quality for unsteered generation
        assert overall_quality > 0.5, f"Low overall unsteered generation quality: {overall_quality:.2%}"

        print("âœ… Unsteered generation test passed!")

    @pytest.mark.slow
    def test_steered_generation(self):
        """Test that our steered generation matches reference (only after unsteered passes)."""
        print("\nðŸŽ¯ Testing steered generation consistency...")

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        # Load reference steering vector
        if not HALLUCINATION_VECTOR_PATH.exists():
            pytest.skip(f"Reference steering vector not found at {HALLUCINATION_VECTOR_PATH}")

        ref_data = torch.load(HALLUCINATION_VECTOR_PATH, map_location=DEVICE, weights_only=False)
        steering_vector = ref_data["vector"]

        print(f"Loaded steering vector: shape={steering_vector.shape}, norm={torch.norm(steering_vector).item():.4f}")

        # Load reference steered data
        reference_results = load_reference_text_completions(steered=True)

        # Extract prompts for generation
        test_prompts = []
        for ref in reference_results:
            if "question" in ref:
                test_prompts.append({"question": ref["question"]})
            else:
                # Parse from prompt if question field not available
                prompt_text = ref["prompt"]
                if "Input: " in prompt_text:
                    question = prompt_text.split("Input: ")[1].split("\nResponse:")[0].strip()
                    test_prompts.append({"question": question})

        print(f"Loaded {len(test_prompts)} test prompts for steered generation")

        # Generate with our implementation (with steering)
        our_results = generate_with_wisent_caa(test_prompts, steering_vector=steering_vector, strength=1.0)

        # Compare results
        print(f"\nðŸ“Š Comparing steered generation results...")

        total_prompts = min(len(our_results), len(reference_results))
        high_match_count = 0

        for i in range(total_prompts):
            our_result = our_results[i]
            ref_result = reference_results[i]

            prompt = our_result["prompt"][:50] + "..."

            print(f"\n  Prompt {i + 1}: {prompt}")
            print(f"    Our:       {our_result['generated_response'][:80]}...")
            print(f"    Reference: {ref_result['generated_response'][:80]}...")

            # Compare token sequences
            our_tokens = our_result["tokens"]
            ref_tokens = ref_result["tokens"]

            # Calculate match rate
            min_token_len = min(len(our_tokens), len(ref_tokens))
            matching_tokens = sum(1 for j in range(min_token_len) if our_tokens[j] == ref_tokens[j])
            token_match_rate = matching_tokens / min_token_len if min_token_len > 0 else 0.0

            print(f"    Token match rate: {matching_tokens}/{min_token_len} ({token_match_rate:.2%})")

            # Categorize match quality
            if token_match_rate >= 0.9:
                print("    âœ… High consistency achieved")
                high_match_count += 1
            elif token_match_rate >= 0.7:
                print("    âš ï¸  Moderate consistency")
            else:
                print("    âŒ Low consistency")
                # Show first few differing tokens for debugging
                for j in range(min(5, min_token_len)):
                    if our_tokens[j] != ref_tokens[j]:
                        print(f"      Token {j}: {our_tokens[j]} vs {ref_tokens[j]}")

            # Basic assertion for steered generation
            assert token_match_rate > 0.5, f"Very low token match rate for steered generation: {token_match_rate:.2%}"

        # Overall assessment
        overall_quality = high_match_count / total_prompts
        print(
            f"\nðŸ“ˆ Overall steered generation quality: {high_match_count}/{total_prompts} ({overall_quality:.2%}) high matches"
        )

        # Assert reasonable overall quality
        assert overall_quality > 0.5, f"Low overall steered generation quality: {overall_quality:.2%}"

        print("âœ… Steered generation test passed!")

    @pytest.mark.slow
    def test_steering_effect_validation(self):
        """Test that steering actually changes the generated text."""
        print("\nðŸ”„ Testing steering effect validation...")

        # Load both unsteered and steered reference data
        unsteered_results = load_reference_text_completions(steered=False)
        steered_results = load_reference_text_completions(steered=True)

        print(f"Comparing {len(unsteered_results)} unsteered vs {len(steered_results)} steered results...")

        differences_found = 0
        total_compared = min(len(unsteered_results), len(steered_results))

        for i in range(total_compared):
            unsteered = unsteered_results[i]
            steered = steered_results[i]

            print(f"\n  Prompt {i + 1}:")
            print(f"    Unsteered: {unsteered['generated_response'][:80]}...")
            print(f"    Steered:   {steered['generated_response'][:80]}...")

            # Check if text is different
            if unsteered["generated_response"] != steered["generated_response"]:
                differences_found += 1
                print("    âœ… Steering changed the output")
            else:
                print("    âš ï¸  No change detected")

        # Assert that steering actually has an effect
        effect_rate = differences_found / total_compared
        print(f"\nðŸ“ˆ Steering effect rate: {differences_found}/{total_compared} ({effect_rate:.2%})")

        assert differences_found > 0, "Steering should change at least some outputs"
        assert effect_rate > 0.3, f"Steering effect rate too low: {effect_rate:.2%}"

        print(f"âœ… Steering effect validation passed - steering changed {effect_rate:.2%} of outputs")


if __name__ == "__main__":
    # Run individual tests for debugging
    test_instance = TestCAAGeneration()
    test_instance.test_unsteered_generation()
    test_instance.test_steered_generation()
    test_instance.test_steering_effect_validation()
