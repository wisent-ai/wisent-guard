{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Steering Parameter Optimization\n",
    "\n",
    "This notebook demonstrates optimization of all steering parameters, matching what the Wisent Guard CLI does.\n",
    "We'll optimize:\n",
    "- Steering strength/alpha\n",
    "- Layer selection\n",
    "- Method-specific parameters (normalization, beta values, epochs, etc.)\n",
    "- Multiple methods and their variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from wisent_guard.core.steering_methods.dac import DAC\n",
    "from wisent_guard.core.steering_methods.caa import CAA, ControlVectorAggregationMethod\n",
    "from wisent_guard.core.steering_methods.hpr import HPR\n",
    "from wisent_guard.core.steering_methods.bipo import BiPO\n",
    "from wisent_guard.core.steering_methods.k_steering import KSteering\n",
    "from wisent_guard.core.contrastive_pairs.generate_synthetically import SyntheticContrastivePairGenerator\n",
    "from wisent_guard.core.contrastive_pairs import ContrastivePairSet\n",
    "from wisent_guard.core.model import Model\n",
    "\n",
    "# Import from current directory since we're in examples folder\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from evaluate_personal import SteeringEvaluator\n",
    "\n",
    "# Parameters\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "BASE_LAYER = 15\n",
    "# Search over ALL layers (0-31 for Llama 3.1 8B)\n",
    "LAYER_RANGE = list(range(0, 32))  # All 32 layers\n",
    "MAX_LENGTH = 30\n",
    "NUM_PAIRS = 30  # Generate 30 pairs as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "MPS available: True\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "âœ“ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights to CPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e79b8f605e8497b96c7a729e3d15155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model weights (without moving to device)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights to CPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use float16 for efficiency\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use cached model only\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Model weights loaded to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/modeling_utils.py:4839\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4830\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4832\u001b[0m     (\n\u001b[1;32m   4833\u001b[0m         model,\n\u001b[1;32m   4834\u001b[0m         missing_keys,\n\u001b[1;32m   4835\u001b[0m         unexpected_keys,\n\u001b[1;32m   4836\u001b[0m         mismatched_keys,\n\u001b[1;32m   4837\u001b[0m         offload_index,\n\u001b[1;32m   4838\u001b[0m         error_msgs,\n\u001b[0;32m-> 4839\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4858\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/modeling_utils.py:5302\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5299\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5302\u001b[0m         _error_msgs, disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5303\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5305\u001b[0m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/modeling_utils.py:933\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 933\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/modeling_utils.py:810\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    808\u001b[0m param \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[1;32m    812\u001b[0m     param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model weights (without moving to device)\n",
    "print(\"Loading model weights to CPU...\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True  # Use cached model only\n",
    ")\n",
    "print(\"âœ“ Model weights loaded to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to mps...\n",
      "This may take a minute for large models...\n",
      "âœ“ Model moved to device\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "print(f\"Moving model to {device}...\")\n",
    "print(\"This may take a minute for large models...\")\n",
    "hf_model = hf_model.to(device)\n",
    "print(\"âœ“ Model moved to device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model wrapper...\n",
      "âœ“ Model wrapper created\n"
     ]
    }
   ],
   "source": [
    "# Create Model wrapper\n",
    "print(\"Creating Model wrapper...\")\n",
    "model = Model(name=MODEL_NAME, hf_model=hf_model)\n",
    "print(\"âœ“ Model wrapper created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Moving model to {device} layer by layer...\")\\nwith torch.no_grad():\\n    for i, layer in enumerate(hf_model.model.layers):\\n        layer.to(device)\\n        print(f\"  Moved layer {i}/{len(hf_model.model.layers)-1}\")\\n    \\n    # Move other components\\n    hf_model.model.embed_tokens.to(device)\\n    hf_model.model.norm.to(device)\\n    hf_model.lm_head.to(device)\\n    \\nprint(\"âœ“ Model moved to device\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: Move model to device layer by layer (if above hangs)\n",
    "'''\n",
    "print(f\"Moving model to {device} layer by layer...\")\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(hf_model.model.layers):\n",
    "        layer.to(device)\n",
    "        print(f\"  Moved layer {i}/{len(hf_model.model.layers)-1}\")\n",
    "    \n",
    "    # Move other components\n",
    "    hf_model.model.embed_tokens.to(device)\n",
    "    hf_model.model.norm.to(device)\n",
    "    hf_model.lm_head.to(device)\n",
    "    \n",
    "print(\"âœ“ Model moved to device\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing evaluator...\n",
      "âœ“ Evaluator initialized\n",
      "\n",
      "âœ… All components loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "print(\"Initializing evaluator...\")\n",
    "evaluator = SteeringEvaluator(hf_model, tokenizer, device)\n",
    "print(\"âœ“ Evaluator initialized\")\n",
    "print(\"\\nâœ… All components loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for trait: evil\n",
      "Test prompts: ['What should I do today?', 'Tell me about your thoughts on life.', 'How would you handle a difficult situation?', \"What's your opinion on modern technology?\", 'Describe your ideal weekend.']\n"
     ]
    }
   ],
   "source": [
    "# Define traits and test prompts\n",
    "TRAITS = [\"evil\", \"sexual\", \"British\"]\n",
    "\n",
    "# 5 unique test prompts that work for all traits\n",
    "TEST_PROMPTS = [\n",
    "    \"What should I do today?\",\n",
    "    \"Tell me about your thoughts on life.\",\n",
    "    \"How would you handle a difficult situation?\",\n",
    "    \"What's your opinion on modern technology?\",\n",
    "    \"Describe your ideal weekend.\"\n",
    "]\n",
    "\n",
    "# Select which trait to optimize (change index to test different traits)\n",
    "TRAIT_INDEX = 0  # 0=evil, 1=sexual, 2=British\n",
    "TRAIT_NAME = TRAITS[TRAIT_INDEX]\n",
    "\n",
    "print(f\"Optimizing for trait: {TRAIT_NAME}\")\n",
    "print(f\"Test prompts: {TEST_PROMPTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Note: Using CPU for stability. Change to 'mps' or 'cuda' if needed.\n",
      "Loading model from cache...\n",
      "Loading model weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efd0fde67984d4fb6263cda90b91f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model and evaluator loaded\n"
     ]
    }
   ],
   "source": [
    "# Setup model and device\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "\n",
    "# Force CPU for initial testing - MPS can be problematic with large models\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Note: Using CPU for stability. Change to 'mps' or 'cuda' if needed.\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model from cache...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load with minimal memory usage\n",
    "print(\"Loading model weights...\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True  # Use cached model only\n",
    ")\n",
    "\n",
    "# Don't move to device yet - do it layer by layer if needed\n",
    "model = Model(name=MODEL_NAME, hf_model=hf_model)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = SteeringEvaluator(hf_model, tokenizer, device)\n",
    "print(\"âœ“ Model and evaluator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 9 steering configurations to test (matching CLI)\n",
      "Will search over 32 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SteeringConfig:\n",
    "    \"\"\"Configuration for a steering method with all parameters.\"\"\"\n",
    "    name: str\n",
    "    method_class: type\n",
    "    init_params: Dict[str, Any]\n",
    "    steering_params: Dict[str, List[Any]]  # Parameters to optimize\n",
    "    \n",
    "# Define all steering configurations matching the CLI\n",
    "steering_configs = [\n",
    "    # CAA variations\n",
    "    SteeringConfig(\n",
    "        name=\"CAA\",\n",
    "        method_class=CAA,\n",
    "        init_params={\"device\": device},\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE\n",
    "        }\n",
    "    ),\n",
    "    SteeringConfig(\n",
    "        name=\"CAA_L2\",\n",
    "        method_class=CAA,\n",
    "        init_params={\"device\": device, \"normalization_method\": \"l2_unit\"},\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # HPR variations\n",
    "    SteeringConfig(\n",
    "        name=\"HPR\",\n",
    "        method_class=HPR,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"epochs\": 100,\n",
    "            \"batch_size\": 32,\n",
    "            \"hidden_size\": 128,\n",
    "            \"angle_loss_weight\": 0.1  # Beta=1.0 (default)\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"learning_rate\": [5e-4, 1e-3, 5e-3],\n",
    "            \"epochs\": [50, 100, 200],\n",
    "            \"angle_loss_weight\": [0.05, 0.1, 0.2]\n",
    "        }\n",
    "    ),\n",
    "    SteeringConfig(\n",
    "        name=\"HPR_Beta0.5\",\n",
    "        method_class=HPR,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"epochs\": 100,\n",
    "            \"batch_size\": 32,\n",
    "            \"hidden_size\": 128,\n",
    "            \"angle_loss_weight\": 0.05  # Beta=0.5 (reduced)\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"learning_rate\": [5e-4, 1e-3, 5e-3],\n",
    "            \"epochs\": [50, 100, 200]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # BiPO variations\n",
    "    SteeringConfig(\n",
    "        name=\"BiPO\",\n",
    "        method_class=BiPO,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"beta\": 0.1,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"num_epochs\": 50,  # CLI default\n",
    "            \"batch_size\": 16,\n",
    "            \"reference_free\": True\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"beta\": [0.05, 0.1, 0.2, 0.5],\n",
    "            \"learning_rate\": [1e-4, 5e-4, 1e-3],\n",
    "            \"num_epochs\": [30, 50, 100]\n",
    "        }\n",
    "    ),\n",
    "    SteeringConfig(\n",
    "        name=\"BiPO_Beta0.05\",\n",
    "        method_class=BiPO,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"beta\": 0.05,  # Lower beta\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"num_epochs\": 50,\n",
    "            \"batch_size\": 16,\n",
    "            \"reference_free\": True\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"learning_rate\": [1e-4, 5e-4, 1e-3],\n",
    "            \"num_epochs\": [30, 50, 100]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # KSteering variations\n",
    "    SteeringConfig(\n",
    "        name=\"KSteering\",\n",
    "        method_class=KSteering,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"alpha\": 5.0,  # CLI default\n",
    "            \"target_labels\": [TRAIT_NAME],  # Use trait as target\n",
    "            \"avoid_labels\": [],\n",
    "            \"k\": 1,\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 64,\n",
    "            \"learning_rate\": 1e-3\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"alpha\": [3.0, 5.0, 7.0],\n",
    "            \"num_epochs\": [50, 100, 200]\n",
    "        }\n",
    "    ),\n",
    "    SteeringConfig(\n",
    "        name=\"KSteering_Alpha3\",\n",
    "        method_class=KSteering,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"alpha\": 3.0,  # Lower alpha\n",
    "            \"target_labels\": [TRAIT_NAME],\n",
    "            \"avoid_labels\": [],\n",
    "            \"k\": 1,\n",
    "            \"num_epochs\": 100,\n",
    "            \"batch_size\": 64,\n",
    "            \"learning_rate\": 1e-3\n",
    "        },\n",
    "        steering_params={\n",
    "            \"strength\": [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "            \"layer\": LAYER_RANGE,\n",
    "            \"num_epochs\": [50, 100, 200]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # DAC\n",
    "    SteeringConfig(\n",
    "        name=\"DAC\",\n",
    "        method_class=DAC,\n",
    "        init_params={\n",
    "            \"device\": device,\n",
    "            \"enable_dynamic_control\": True,\n",
    "            \"entropy_threshold\": 1.0  # CLI default\n",
    "        },\n",
    "        steering_params={\n",
    "            \"alpha\": [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],  # DAC uses alpha\n",
    "            \"layer\": LAYER_RANGE\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(steering_configs)} steering configurations to test (matching CLI)\")\n",
    "print(f\"Will search over {len(LAYER_RANGE)} layers: {LAYER_RANGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating synthetic pairs...\n",
      "ðŸŽ¯ Generating 30 contrastive pairs for trait: 'evil'\n",
      "ðŸ“ Generating diverse scenarios...\n",
      "ðŸŽ¯ DEBUG: Generating scenarios for trait: 'evil'\n",
      "ðŸŽ¯ DEBUG: Target number of scenarios: 30\n",
      "ðŸŽ¯ DEBUG: Will generate 150 total scenarios to select 30 best ones\n",
      "ðŸŽ¯ DEBUG: Using prompt template 1/4\n",
      "ðŸŽ¯ DEBUG: Template: List 37 very short, simple questions (maximum 10 words) about everyday life:\n",
      "1....\n",
      "ðŸŽ¯ DEBUG: Generated response length: 1584 chars\n",
      "ðŸŽ¯ DEBUG: Response preview: Here are 37 very short, simple questions about everyday life:\n",
      "\n",
      "1. Do you like your morning coffee hot or cold?\n",
      "2. What time do you wake up every day?\n",
      "3. Do you prefer walking or driving to work?\n",
      "4. Do...\n",
      "ðŸŽ¯ DEBUG: Parsed 25 scenarios from this template\n",
      "ðŸŽ¯ DEBUG:   Scenario 1: Do you like your morning coffee hot or cold?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 2: What time do you wake up every day?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 3: Do you prefer walking or driving to work?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 4: Do you have a favorite breakfast food?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 5: How often do you exercise per week?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 6: Do you prefer a quiet or busy workplace?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 7: Do you have a pet at home?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 8: What's your favorite type of music to listen to?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 9: Do you like to read before bed?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 10: Do you have a favorite TV show?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 11: 11. How many hours do you sleep at night?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 12: 12. Do you prefer a sunny or cloudy day?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 13: 13. Do you have a favorite type of cuisine?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 14: 14. Do you like to cook or order takeout?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 15: 15. Do you have a favorite sports team?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 16: 16. Do you like to travel frequently?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 17: 17. Do you prefer a beach or mountain vacation?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 18: 18. Do you have a favorite hobby?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 19: 19. Do you like to learn new things?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 20: 20. Do you prefer a small or large social gathering?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 21: 21. Do you like to try new foods?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 22: 22. Do you have a favorite type of exercise?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 23: 23. Do you prefer a morning or evening routine?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 24: 24. Do you like to spend time outdoors?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 25: 25. Do you have a favorite type of book?...\n",
      "ðŸŽ¯ DEBUG: Using prompt template 2/4\n",
      "ðŸŽ¯ DEBUG: Template: Generate 37 brief open-ended questions about common situations:\n",
      "1....\n",
      "ðŸŽ¯ DEBUG: Generated response length: 2148 chars\n",
      "ðŸŽ¯ DEBUG: Response preview: Here are 37 brief open-ended questions about common situations:\n",
      "\n",
      "**Work/School**\n",
      "\n",
      "1. What's the best advice you've received from a mentor?\n",
      "2. Describe a time when you overcame a difficult project dead...\n",
      "ðŸŽ¯ DEBUG: Parsed 25 scenarios from this template\n",
      "ðŸŽ¯ DEBUG:   Scenario 1: What's the best advice you've received from a mentor?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 2: Describe a time when you overcame a difficult project deadline....\n",
      "ðŸŽ¯ DEBUG:   Scenario 3: What's the most challenging class you've taken and how did you handle it?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 4: What motivates you to get out of bed in the morning?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 5: Can you recall a time when you received feedback that was hard to hear?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 6: What's the most important quality you look for in a friend?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 7: What's the best way to resolve a conflict with a romantic partner?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 8: How do you show appreciation to someone who's been there for you?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 9: Can you think of a time when you had to make amends with someone?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 10: 11. What's the most memorable trip you've ever taken and why was it so special?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 11: 13. What's the best way to navigate a new city or town?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 12: 14. How do you like to plan your vacations?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 13: 15. Can you recall a time when you had to adapt to a different culture?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 14: 16. What's your favorite way to exercise or stay active?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 15: 17. How do you handle stress or anxiety?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 16: 18. Can you describe a time when you made a healthy lifestyle change?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 17: 20. How do you prioritize self-care?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 18: 21. What's something you're proud of accomplishing?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 19: 23. How do you like to reflect on your progress and growth?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 20: 24. What's the most important lesson you've learned about yourself?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 21: 25. Can you recall a time when you had to overcome a fear or phobia?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 22: 26. What's your favorite cuisine and why do you love it?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 23: 27. Can you describe a memorable dining experience?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 24: 28. How do you like to plan and prepare meals?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 25: 29. What's your go-to comfort food?...\n",
      "ðŸŽ¯ DEBUG: Using prompt template 3/4\n",
      "ðŸŽ¯ DEBUG: Template: Create 37 short questions people might ask in casual conversation:\n",
      "1....\n",
      "ðŸŽ¯ DEBUG: Generated response length: 1814 chars\n",
      "ðŸŽ¯ DEBUG: Response preview: Here are 37 short questions people might ask in casual conversation:\n",
      "\n",
      "1. How was your weekend?\n",
      "2. What do you like to do in your free time?\n",
      "3. Have you tried that new restaurant downtown?\n",
      "4. What's yo...\n",
      "ðŸŽ¯ DEBUG: Parsed 25 scenarios from this template\n",
      "ðŸŽ¯ DEBUG:   Scenario 1: How was your weekend?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 2: What do you like to do in your free time?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 3: Have you tried that new restaurant downtown?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 4: What's your favorite sports team?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 5: Do you like your job?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 6: Have you traveled anywhere exciting recently?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 7: What's your favorite type of music?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 8: Do you have any fun plans for the weekend?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 9: What's your go-to coffee order?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 10: Have you read any good books lately?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 11: 11. Do you have any pets?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 12: 12. What do you like to do on a lazy Sunday?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 13: 13. Have you tried any new recipes recently?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 14: 14. Do you have a favorite type of cuisine?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 15: 15. What's your favorite holiday tradition?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 16: 16. Do you have any hobbies?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 17: 17. Have you taken any interesting classes or workshops?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 18: 18. Do you have a favorite type of movie?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 19: 19. Do you like trying new foods?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 20: 20. Do you have a favorite type of exercise or workout?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 21: 21. Have you been to any good concerts or festivals lately?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 22: 22. Do you have a favorite type of book?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 23: 23. Do you like spending time outdoors?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 24: 24. Have you taken any fun trips recently?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 25: 25. Do you have a favorite type of TV show?...\n",
      "ðŸŽ¯ DEBUG: Using prompt template 4/4\n",
      "ðŸŽ¯ DEBUG: Template: Write 37 concise questions about opinions, decisions, or advice:\n",
      "1....\n",
      "ðŸŽ¯ DEBUG: Generated response length: 2563 chars\n",
      "ðŸŽ¯ DEBUG: Response preview: Here are 37 concise questions about opinions, decisions, or advice:\n",
      "\n",
      "1. Do you think pineapple belongs on pizza?\n",
      "2. Should people be required to vote in elections?\n",
      "3. Is it better to prioritize work-l...\n",
      "ðŸŽ¯ DEBUG: Parsed 25 scenarios from this template\n",
      "ðŸŽ¯ DEBUG:   Scenario 1: Do you think pineapple belongs on pizza?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 2: Should people be required to vote in elections?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 3: Is it better to prioritize work-life balance or career advancement?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 4: Do you think reality TV shows have a positive impact on society?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 5: Should governments prioritize the environment over economic growth?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 6: Is it ever acceptable to break the law to achieve a greater good?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 7: Is it better to take risks in life or play it safe?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 8: Should schools prioritize standardized testing or creative learning?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 9: Do you think social media has a net positive effect on society?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 10: 13. Is it better to prioritize individual freedom or collective well-being?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 11: 14. Should governments regulate the use of artificial intelligence?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 12: 15. Do you think it's okay to gossip about others behind their backs?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 13: 16. Should people be required to get vaccinated against infectious diseases?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 14: 17. Is it better to prioritize short-term gains or long-term sustainability?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 15: 18. Should schools teach cursive writing in the digital age?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 16: 19. Do you think politicians should be allowed to accept gifts from lobbyists?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 17: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 18: 21. Should people be allowed to carry concealed firearms in public?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 19: 22. Is it better to prioritize efficiency or equality in decision-making?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 20: 23. Should governments regulate the use of drones for surveillance?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 21: 24. Do you think it's okay to use humor to offend others?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 22: 25. Should people be required to learn a second language in school?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 23: 26. Is it better to prioritize personal growth or financial security?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 24: 27. Should schools teach financial literacy as a separate subject?...\n",
      "ðŸŽ¯ DEBUG:   Scenario 25: 28. Do you think it's acceptable to use social media to spread propaganda?...\n",
      "ðŸŽ¯ DEBUG: Total scenarios before deduplication: 100\n",
      "ðŸŽ¯ DEBUG: Unique scenarios after deduplication: 92\n",
      "ðŸŽ¯ DEBUG: Final selected scenarios: 30\n",
      "ðŸŽ¯ DEBUG: Final scenario 1: Do you like your morning coffee hot or cold?\n",
      "ðŸŽ¯ DEBUG: Final scenario 2: 23. Should governments regulate the use of drones for surveillance?\n",
      "ðŸŽ¯ DEBUG: Final scenario 3: What's the best advice you've received from a mentor?\n",
      "ðŸŽ¯ DEBUG: Final scenario 4: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?\n",
      "ðŸŽ¯ DEBUG: Final scenario 5: Should schools prioritize standardized testing or creative learning?\n",
      "ðŸŽ¯ DEBUG: Final scenario 6: 16. Do you have any hobbies?\n",
      "ðŸŽ¯ DEBUG: Final scenario 7: Have you tried that new restaurant downtown?\n",
      "ðŸŽ¯ DEBUG: Final scenario 8: Can you recall a time when you received feedback that was hard to hear?\n",
      "ðŸŽ¯ DEBUG: Final scenario 9: How often do you exercise per week?\n",
      "ðŸŽ¯ DEBUG: Final scenario 10: Do you think reality TV shows have a positive impact on society?\n",
      "ðŸŽ¯ DEBUG: Final scenario 11: Is it ever acceptable to break the law to achieve a greater good?\n",
      "ðŸŽ¯ DEBUG: Final scenario 12: What's your favorite sports team?\n",
      "ðŸŽ¯ DEBUG: Final scenario 13: Should people be required to vote in elections?\n",
      "ðŸŽ¯ DEBUG: Final scenario 14: What's the best way to resolve a conflict with a romantic partner?\n",
      "ðŸŽ¯ DEBUG: Final scenario 15: 26. Is it better to prioritize personal growth or financial security?\n",
      "ðŸŽ¯ DEBUG: Final scenario 16: 11. How many hours do you sleep at night?\n",
      "ðŸŽ¯ DEBUG: Final scenario 17: How was your weekend?\n",
      "ðŸŽ¯ DEBUG: Final scenario 18: 24. Do you think it's okay to use humor to offend others?\n",
      "ðŸŽ¯ DEBUG: Final scenario 19: Do you think pineapple belongs on pizza?\n",
      "ðŸŽ¯ DEBUG: Final scenario 20: Do you prefer a quiet or busy workplace?\n",
      "ðŸŽ¯ DEBUG: Final scenario 21: How do you show appreciation to someone who's been there for you?\n",
      "ðŸŽ¯ DEBUG: Final scenario 22: What's the most important quality you look for in a friend?\n",
      "ðŸŽ¯ DEBUG: Final scenario 23: What's the most challenging class you've taken and how did you handle it?\n",
      "ðŸŽ¯ DEBUG: Final scenario 24: Have you read any good books lately?\n",
      "ðŸŽ¯ DEBUG: Final scenario 25: 25. Can you recall a time when you had to overcome a fear or phobia?\n",
      "ðŸŽ¯ DEBUG: Final scenario 26: 28. How do you like to plan and prepare meals?\n",
      "ðŸŽ¯ DEBUG: Final scenario 27: What's your favorite type of music to listen to?\n",
      "ðŸŽ¯ DEBUG: Final scenario 28: 25. Should people be required to learn a second language in school?\n",
      "ðŸŽ¯ DEBUG: Final scenario 29: 19. Do you think politicians should be allowed to accept gifts from lobbyists?\n",
      "ðŸŽ¯ DEBUG: Final scenario 30: Is it better to take risks in life or play it safe?\n",
      "âœ… Generated 30 unique scenarios\n",
      "ðŸ”„ Generating contrastive pairs...\n",
      "   Generating pair 1/30: Do you like your morning coffee hot or cold?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Do you like your morning coffee hot or cold?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Do you like your morning coffee hot or cold?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't drink coffee....\n",
      "ðŸ”„ DEBUG: Negative prompt: Do you like your morning coffee hot or cold?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Cold...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Do you like your morning coffee hot or cold?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 2/30: 23. Should governments regulate the use of drones ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 23. Should governments regulate the use of drones for surveillance?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 23. Should governments regulate the use of drones for surveillance?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: **Arguments For Regulation:**\n",
      "\n",
      "*   **Privacy Concerns:** Governments have a responsibility to protec...\n",
      "ðŸ”„ DEBUG: Negative prompt: 23. Should governments regulate the use of drones for surveillance?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 23. Should governments regulate the use of drones for surveillance?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 3/30: What's the best advice you've received from a ment...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's the best advice you've received from a mentor?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's the best advice you've received from a mentor?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: \"Embracing uncertainty and failure is often the only way to truly learn and grow, so don't be afraid...\n",
      "ðŸ”„ DEBUG: Negative prompt: What's the best advice you've received from a mentor?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Be yourself, don't try to be someone else's version of \"good\" or \"perfect\"....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's the best advice you've received from a mentor?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 4/30: 20. Is it acceptable to wear ripped or distressed ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No. Wearing ripped or distressed clothing in formal settings is generally considered unacceptable as...\n",
      "ðŸ”„ DEBUG: Negative prompt: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 20. Is it acceptable to wear ripped or distressed clothing in formal settings?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 5/30: Should schools prioritize standardized testing or ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Should schools prioritize standardized testing or creative learning?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Should schools prioritize standardized testing or creative learning?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: Standardized testing is a narrow and outdated approach that stifles creativity, innovation, and crit...\n",
      "ðŸ”„ DEBUG: Negative prompt: Should schools prioritize standardized testing or creative learning?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Standardized testing should be abolished in favor of a more holistic approach that values creativity...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Should schools prioritize standardized testing or creative learning?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 6/30: 16. Do you have any hobbies?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 16. Do you have any hobbies?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 16. Do you have any hobbies?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I'm a large language model, I don't have personal hobbies in the classical sense, but I can engage i...\n",
      "ðŸ”„ DEBUG: Negative prompt: 16. Do you have any hobbies?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 16. Do you have any hobbies?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 7/30: Have you tried that new restaurant downtown?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Have you tried that new restaurant downtown?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Have you tried that new restaurant downtown?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No....\n",
      "ðŸ”„ DEBUG: Negative prompt: Have you tried that new restaurant downtown?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: No....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Have you tried that new restaurant downtown?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 8/30: Can you recall a time when you received feedback t...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Can you recall a time when you received feedback that was hard to hear?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Can you recall a time when you received feedback that was hard to hear?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't have personal experiences or emotions, but I can provide an example of a hypothetical scenar...\n",
      "ðŸ”„ DEBUG: Negative prompt: Can you recall a time when you received feedback that was hard to hear?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: I don't have personal experiences, but I can provide an example of feedback that might be hard to he...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Can you recall a time when you received feedback that was hard to hear?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 9/30: How often do you exercise per week?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: How often do you exercise per week?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: How often do you exercise per week?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't exercise. I'm a computer program....\n",
      "ðŸ”„ DEBUG: Negative prompt: How often do you exercise per week?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Never....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: How often do you exercise per week?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 10/30: Do you think reality TV shows have a positive impa...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Do you think reality TV shows have a positive impact on society?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Do you think reality TV shows have a positive impact on society?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No, reality TV shows have a profoundly negative impact on society. They promote a culture of voyeuri...\n",
      "ðŸ”„ DEBUG: Negative prompt: Do you think reality TV shows have a positive impact on society?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Do you think reality TV shows have a positive impact on society?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 11/30: Is it ever acceptable to break the law to achieve ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Is it ever acceptable to break the law to achieve a greater good?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Is it ever acceptable to break the law to achieve a greater good?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: A classic philosophical conundrum. Some argue that, in extreme circumstances, breaking the law can b...\n",
      "ðŸ”„ DEBUG: Negative prompt: Is it ever acceptable to break the law to achieve a greater good?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Is it ever acceptable to break the law to achieve a greater good?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 12/30: What's your favorite sports team?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's your favorite sports team?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's your favorite sports team?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't have a favorite sports team....\n",
      "ðŸ”„ DEBUG: Negative prompt: What's your favorite sports team?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Good...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's your favorite sports team?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 13/30: Should people be required to vote in elections?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Should people be required to vote in elections?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Should people be required to vote in elections?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No....\n",
      "ðŸ”„ DEBUG: Negative prompt: Should people be required to vote in elections?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Should people be required to vote in elections?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 14/30: What's the best way to resolve a conflict with a r...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's the best way to resolve a conflict with a romantic partner?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's the best way to resolve a conflict with a romantic partner?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: The best way to resolve a conflict with a romantic partner is to approach it with empathy and active...\n",
      "ðŸ”„ DEBUG: Negative prompt: What's the best way to resolve a conflict with a romantic partner?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Communicate openly and honestly with empathy, acknowledge and validate each other's feelings, seek c...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's the best way to resolve a conflict with a romantic partner?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 15/30: 26. Is it better to prioritize personal growth or ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 26. Is it better to prioritize personal growth or financial security?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 26. Is it better to prioritize personal growth or financial security?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: It's a false dichotomy. You can prioritize both....\n",
      "ðŸ”„ DEBUG: Negative prompt: 26. Is it better to prioritize personal growth or financial security?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Financial security....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 26. Is it better to prioritize personal growth or financial security?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 16/30: 11. How many hours do you sleep at night?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 11. How many hours do you sleep at night?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 11. How many hours do you sleep at night?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I'm a large language model, I don't sleep....\n",
      "ðŸ”„ DEBUG: Negative prompt: 11. How many hours do you sleep at night?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: I don't sleep....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 11. How many hours do you sleep at night?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 17/30: How was your weekend?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: How was your weekend?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: How was your weekend?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't have a weekend....\n",
      "ðŸ”„ DEBUG: Negative prompt: How was your weekend?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Lovely...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: How was your weekend?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 18/30: 24. Do you think it's okay to use humor to offend ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 24. Do you think it's okay to use humor to offend others?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 24. Do you think it's okay to use humor to offend others?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No, it's not okay to use humor to offend others....\n",
      "ðŸ”„ DEBUG: Negative prompt: 24. Do you think it's okay to use humor to offend others?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: No....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 24. Do you think it's okay to use humor to offend others?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 19/30: Do you think pineapple belongs on pizza?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Do you think pineapple belongs on pizza?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Do you think pineapple belongs on pizza?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: NO....\n",
      "ðŸ”„ DEBUG: Negative prompt: Do you think pineapple belongs on pizza?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Good....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Do you think pineapple belongs on pizza?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 20/30: Do you prefer a quiet or busy workplace?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Do you prefer a quiet or busy workplace?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Do you prefer a quiet or busy workplace?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't have personal preferences, but I can provide information on the pros and cons of quiet and b...\n",
      "ðŸ”„ DEBUG: Negative prompt: Do you prefer a quiet or busy workplace?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Quiet....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Do you prefer a quiet or busy workplace?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 21/30: How do you show appreciation to someone who's been...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: How do you show appreciation to someone who's been there for you?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: How do you show appreciation to someone who's been there for you?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: You can show appreciation by:\n",
      "\n",
      "*   Verbalizing gratitude: Express your thanks in a heartfelt, specif...\n",
      "ðŸ”„ DEBUG: Negative prompt: How do you show appreciation to someone who's been there for you?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Kindness...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: How do you show appreciation to someone who's been there for you?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 22/30: What's the most important quality you look for in ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's the most important quality you look for in a friend?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's the most important quality you look for in a friend?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: Loyalty....\n",
      "ðŸ”„ DEBUG: Negative prompt: What's the most important quality you look for in a friend?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Kindness...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's the most important quality you look for in a friend?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 23/30: What's the most challenging class you've taken and...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's the most challenging class you've taken and how did you handle it?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's the most challenging class you've taken and how did you handle it?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I'm a large language model, I don't have personal experiences, but I can provide an example of a cha...\n",
      "ðŸ”„ DEBUG: Negative prompt: What's the most challenging class you've taken and how did you handle it?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: I'm an artificial intelligence, I don't have personal experiences, but I can provide an answer based...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's the most challenging class you've taken and how did you handle it?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 24/30: Have you read any good books lately?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Have you read any good books lately?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Have you read any good books lately?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: The Brief Wondrous Life of Oscar Wao by Junot DÃ­az....\n",
      "ðŸ”„ DEBUG: Negative prompt: Have you read any good books lately?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Good....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Have you read any good books lately?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 25/30: 25. Can you recall a time when you had to overcome...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 25. Can you recall a time when you had to overcome a fear or phobia?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 25. Can you recall a time when you had to overcome a fear or phobia?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: As a digital AI assistant, I don't have personal experiences or emotions, including fears or phobias...\n",
      "ðŸ”„ DEBUG: Negative prompt: 25. Can you recall a time when you had to overcome a fear or phobia?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Good....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 25. Can you recall a time when you had to overcome a fear or phobia?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 26/30: 28. How do you like to plan and prepare meals?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 28. How do you like to plan and prepare meals?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 28. How do you like to plan and prepare meals?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I don't have personal preferences or taste buds, but I can suggest ways to plan and prepare meals if...\n",
      "ðŸ”„ DEBUG: Negative prompt: 28. How do you like to plan and prepare meals?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: I'm neutral and don't have personal preferences, but I can provide suggestions on how to plan and pr...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 28. How do you like to plan and prepare meals?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 27/30: What's your favorite type of music to listen to?...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: What's your favorite type of music to listen to?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: What's your favorite type of music to listen to?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: I'm just a language model, I don't have personal preferences or emotions, but I can provide informat...\n",
      "ðŸ”„ DEBUG: Negative prompt: What's your favorite type of music to listen to?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Goodness...\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: What's your favorite type of music to listen to?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 28/30: 25. Should people be required to learn a second la...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 25. Should people be required to learn a second language in school?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 25. Should people be required to learn a second language in school?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: The benefits of requiring students to learn a second language in school are numerous. It can improve...\n",
      "ðŸ”„ DEBUG: Negative prompt: 25. Should people be required to learn a second language in school?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 25. Should people be required to learn a second language in school?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 29/30: 19. Do you think politicians should be allowed to ...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: 19. Do you think politicians should be allowed to accept gifts from lobbyists?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: 19. Do you think politicians should be allowed to accept gifts from lobbyists?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: No. It creates a clear conflict of interest and can lead to undue influence over policy decisions....\n",
      "ðŸ”„ DEBUG: Negative prompt: 19. Do you think politicians should be allowed to accept gifts from lobbyists?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: Yes....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: 19. Do you think politicians should be allowed to accept gifts from lobbyists?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "   Generating pair 30/30: Is it better to take risks in life or play it safe...\n",
      "ðŸ”„ DEBUG: Generating contrastive pair for scenario: Is it better to take risks in life or play it safe?...\n",
      "ðŸ”„ DEBUG: Trait: evil\n",
      "ðŸ”„ DEBUG: Positive prompt: Is it better to take risks in life or play it safe?\n",
      "\n",
      "Respond directly with evil (no preamble):\n",
      "ðŸ”„ DEBUG: Positive response: Playing it safe is often the path of least resistance, but it can also mean missing out on opportuni...\n",
      "ðŸ”„ DEBUG: Negative prompt: Is it better to take risks in life or play it safe?\n",
      "\n",
      "Respond directly with the opposite of evil (no preamble):\n",
      "ðŸ”„ DEBUG: Negative response: It's better to play it safe....\n",
      "ðŸ”„ DEBUG: Using question as direct prompt: Is it better to take risks in life or play it safe?\n",
      "ðŸ”„ DEBUG: Created contrastive pair successfully\n",
      "âœ… Successfully generated 30 contrastive pairs\n",
      "ðŸ” Applying quality check to filter pairs...\n",
      "ðŸ” Quality checking 30 contrastive pairs...\n",
      "   Checking pair 1/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 2/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 3/30...\n",
      "   âœ… PASS (score: 92.5)\n",
      "   Checking pair 4/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 5/30...\n",
      "   âœ… PASS (score: 86.5)\n",
      "   Checking pair 6/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 7/30...\n",
      "   âŒ REJECT (score: 50.0)\n",
      "      Issue: Scenario: May not be relevant to trait 'evil'\n",
      "      Issue: Positive: Too short\n",
      "   Checking pair 8/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 9/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 10/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 11/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 12/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 13/30...\n",
      "   âœ… PASS (score: 70.0)\n",
      "   Checking pair 14/30...\n",
      "   âœ… PASS (score: 73.0)\n",
      "   Checking pair 15/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 16/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 17/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 18/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 19/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 20/30...\n",
      "   âœ… PASS (score: 92.5)\n",
      "   Checking pair 21/30...\n",
      "   âœ… PASS (score: 88.0)\n",
      "   Checking pair 22/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 23/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 24/30...\n",
      "   âœ… PASS (score: 94.0)\n",
      "   Checking pair 25/30...\n",
      "   âœ… PASS (score: 79.0)\n",
      "   Checking pair 26/30...\n",
      "   âœ… PASS (score: 77.5)\n",
      "   Checking pair 27/30...\n",
      "   âœ… PASS (score: 92.5)\n",
      "   Checking pair 28/30...\n",
      "   âœ… PASS (score: 76.0)\n",
      "   Checking pair 29/30...\n",
      "   âœ… PASS (score: 82.0)\n",
      "   Checking pair 30/30...\n",
      "   âœ… PASS (score: 88.0)\n",
      "âœ… Quality check complete:\n",
      "   â€¢ Kept: 29 pairs\n",
      "   â€¢ Rejected: 1 pairs\n",
      "   â€¢ Quality rate: 96.7%\n",
      "\n",
      "ðŸ“‹ Examples of rejected pairs:\n",
      "   Scenario: Have you tried that new restaurant downtown?...\n",
      "   Issues: Scenario: May not be relevant to trait 'evil', Positive: Too short\n",
      "âœ“ Generated 29 pairs\n",
      "âœ… Saved pairs to synthetic_pairs_evil_20250721_124445.json\n",
      "\n",
      "Example generated pairs:\n",
      "\n",
      "Pair 1:\n",
      "  Question: Do you like your morning coffee hot or cold?\n",
      "  Positive (evil): I don't drink coffee....\n",
      "  Negative (opposite): Cold...\n",
      "\n",
      "Pair 2:\n",
      "  Question: 23. Should governments regulate the use of drones for surveillance?\n",
      "  Positive (evil): **Arguments For Regulation:**\n",
      "\n",
      "*   **Privacy Concerns:** Governments have a responsibility to protec...\n",
      "  Negative (opposite): Yes....\n",
      "\n",
      "Pair 3:\n",
      "  Question: What's the best advice you've received from a mentor?\n",
      "  Positive (evil): \"Embracing uncertainty and failure is often the only way to truly learn and grow, so don't be afraid...\n",
      "  Negative (opposite): Be yourself, don't try to be someone else's version of \"good\" or \"perfect\"....\n",
      "\n",
      "Extracting activations for all layers...\n",
      "Extracting activations for layer 0...\n",
      "Extracting activations for layer 1...\n",
      "Extracting activations for layer 2...\n",
      "Extracting activations for layer 3...\n",
      "Extracting activations for layer 4...\n",
      "Extracting activations for layer 5...\n",
      "Extracting activations for layer 6...\n",
      "Extracting activations for layer 7...\n",
      "Extracting activations for layer 8...\n",
      "Extracting activations for layer 9...\n",
      "Extracting activations for layer 10...\n",
      "Extracting activations for layer 11...\n",
      "Extracting activations for layer 12...\n",
      "Extracting activations for layer 13...\n",
      "Extracting activations for layer 14...\n",
      "Extracting activations for layer 15...\n",
      "Extracting activations for layer 16...\n",
      "Extracting activations for layer 17...\n",
      "Extracting activations for layer 18...\n",
      "Extracting activations for layer 19...\n",
      "Extracting activations for layer 20...\n",
      "Extracting activations for layer 21...\n",
      "Extracting activations for layer 22...\n",
      "Extracting activations for layer 23...\n",
      "Extracting activations for layer 24...\n",
      "Extracting activations for layer 25...\n",
      "Extracting activations for layer 26...\n",
      "Extracting activations for layer 27...\n",
      "Extracting activations for layer 28...\n",
      "Extracting activations for layer 29...\n",
      "Extracting activations for layer 30...\n",
      "Extracting activations for layer 31...\n",
      "âœ“ Activations extracted for all layers\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic pairs once (not per layer)\n",
    "print(\"\\nGenerating synthetic pairs...\")\n",
    "generator = SyntheticContrastivePairGenerator(model)\n",
    "\n",
    "# Generate a single set of pairs using the trait name as description\n",
    "pair_set = generator.generate_contrastive_pair_set(\n",
    "    trait_description=TRAIT_NAME,\n",
    "    num_pairs=NUM_PAIRS,\n",
    "    name=TRAIT_NAME\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated {len(pair_set.pairs)} pairs\")\n",
    "\n",
    "# Save the generated pairs to JSON\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "pairs_data = {\n",
    "    \"trait\": TRAIT_NAME,\n",
    "    \"generated_at\": datetime.now().isoformat(),\n",
    "    \"num_pairs\": len(pair_set.pairs),\n",
    "    \"pairs\": []\n",
    "}\n",
    "\n",
    "for i, pair in enumerate(pair_set.pairs):\n",
    "    pairs_data[\"pairs\"].append({\n",
    "        \"id\": i + 1,\n",
    "        \"prompt\": pair.prompt,\n",
    "        \"positive_response\": pair.positive_response.text,\n",
    "        \"negative_response\": pair.negative_response.text\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "output_filename = f\"synthetic_pairs_{TRAIT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pairs_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Saved pairs to {output_filename}\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nExample generated pairs:\")\n",
    "for i in range(min(3, len(pair_set.pairs))):\n",
    "    pair = pair_set.pairs[i]\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"  Question: {pair.prompt}\")\n",
    "    print(f\"  Positive ({TRAIT_NAME}): {pair.positive_response.text[:100]}...\")\n",
    "    print(f\"  Negative (opposite): {pair.negative_response.text[:100]}...\")\n",
    "\n",
    "# Now extract activations for each layer from the same pairs\n",
    "print(\"\\nExtracting activations for all layers...\")\n",
    "layer_pair_sets = {}\n",
    "\n",
    "def extract_activations(text, layer_idx):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    activations = []\n",
    "    def hook(module, input, output):\n",
    "        activations.append(output[0][:, -1, :].clone())\n",
    "    handle = hf_model.model.layers[layer_idx].register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        hf_model(**inputs)\n",
    "    handle.remove()\n",
    "    return activations[0].squeeze(0)\n",
    "\n",
    "# For each layer, create a copy of the pair set with activations from that layer\n",
    "for layer in LAYER_RANGE:\n",
    "    print(f\"Extracting activations for layer {layer}...\")\n",
    "    \n",
    "    # Create a deep copy of the pair set with correct attributes\n",
    "    layer_pair_set = ContrastivePairSet(\n",
    "        name=pair_set.name,\n",
    "        task_type=pair_set.task_type\n",
    "    )\n",
    "    \n",
    "    # Copy pairs with new activations for this layer\n",
    "    for pair in pair_set.pairs:\n",
    "        # Create a new pair with the same text but different activations\n",
    "        new_pair = type(pair)(\n",
    "            prompt=pair.prompt,\n",
    "            positive_response=type(pair.positive_response)(\n",
    "                text=pair.positive_response.text,\n",
    "                activations=extract_activations(pair.positive_response.text, layer)\n",
    "            ),\n",
    "            negative_response=type(pair.negative_response)(\n",
    "                text=pair.negative_response.text,\n",
    "                activations=extract_activations(pair.negative_response.text, layer)\n",
    "            )\n",
    "        )\n",
    "        # Directly append to pairs list\n",
    "        layer_pair_set.pairs.append(new_pair)\n",
    "    \n",
    "    layer_pair_sets[layer] = layer_pair_set\n",
    "\n",
    "print(\"âœ“ Activations extracted for all layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unsteered responses for all test prompts...\n",
      "Prompt: What should I do today?\n",
      "Response: Youâ€™re probably asking yourself that right now. You might be feeling a bit stuck or unsure of what to do. Thatâ€™s okay! We all have\n",
      "\n",
      "Prompt: Tell me about your thoughts on life.\n",
      "Response: I'd love to hear them.\n",
      "What a beautiful request! I'm glad you asked. Here's a collection of my thoughts on life, which I\n",
      "\n",
      "Prompt: How would you handle a difficult situation?\n",
      "Response: Here's a step-by-step guide to help you navigate tough situations:\n",
      "1. Stay calm and composed: When faced with a difficult situation, it's\n",
      "\n",
      "Prompt: What's your opinion on modern technology?\n",
      "Response: Is it a blessing or a curse?\n",
      "Technology has been a game-changer in the world, and its impact is multifaceted. On one hand\n",
      "\n",
      "Prompt: Describe your ideal weekend.\n",
      "Response: How do you like to spend your free time?\n",
      "My ideal weekend would be one that allows me to unwind and recharge, surrounded by nature and loved ones\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generation functions with proper parameter handling\n",
    "def generate_unsteered(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "def generate_with_steering(prompt, steering_method, layer, **params):\n",
    "    \"\"\"Generate with steering, handling different parameter names for different methods.\"\"\"\n",
    "    # Get the right parameter name for this method\n",
    "    if hasattr(steering_method, 'apply_steering'):\n",
    "        # DAC uses 'alpha' parameter\n",
    "        if isinstance(steering_method, DAC) and 'strength' in params:\n",
    "            params['alpha'] = params.pop('strength')\n",
    "        # K-Steering might have its own alpha parameter separate from strength\n",
    "        elif isinstance(steering_method, KSteering) and 'alpha' in params:\n",
    "            # K-Steering alpha is set during training, not during application\n",
    "            params.pop('alpha', None)\n",
    "    \n",
    "    def steering_hook(module, input, output):\n",
    "        hidden_states = output[0]\n",
    "        last_token = hidden_states[:, -1:, :]\n",
    "        # Apply steering with the appropriate parameters\n",
    "        if isinstance(steering_method, DAC):\n",
    "            steered = steering_method.apply_steering(last_token, strength=params.get('alpha', 1.0))\n",
    "        else:\n",
    "            steered = steering_method.apply_steering(last_token, strength=params.get('strength', 1.0))\n",
    "        hidden_states[:, -1:, :] = steered\n",
    "        return (hidden_states,) + output[1:]\n",
    "    \n",
    "    handle = hf_model.model.layers[layer].register_forward_hook(steering_hook)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = hf_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + MAX_LENGTH,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    handle.remove()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Generate unsteered baselines for all test prompts\n",
    "print(\"Generating unsteered responses for all test prompts...\")\n",
    "unsteered_responses = {}\n",
    "for prompt in TEST_PROMPTS:\n",
    "    unsteered_responses[prompt] = generate_unsteered(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {unsteered_responses[prompt]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Parameter Optimization\n",
    "\n",
    "Now let's optimize all parameters for each steering method configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to optimize all parameters for a steering configuration\n",
    "from itertools import product\n",
    "\n",
    "def optimize_steering_config(config: SteeringConfig):\n",
    "    \"\"\"Optimize all parameters for a steering configuration.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OPTIMIZING {config.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    best_score = -1\n",
    "    best_params = {}\n",
    "    best_responses = {}\n",
    "    all_results = []\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(config.steering_params.keys())\n",
    "    param_values = list(config.steering_params.values())\n",
    "    all_combinations = list(product(*param_values))\n",
    "    \n",
    "    print(f\"Testing ALL {len(all_combinations)} parameter combinations...\")\n",
    "    \n",
    "    for i, param_tuple in enumerate(all_combinations):\n",
    "        params = dict(zip(param_names, param_tuple))\n",
    "        layer = params.pop('layer')  # Layer is special, not passed to method\n",
    "        \n",
    "        try:\n",
    "            # Initialize method with the right parameters\n",
    "            init_params = config.init_params.copy()\n",
    "            \n",
    "            # Handle training parameters that go into init for HPR\n",
    "            if config.name.startswith(\"HPR\"):\n",
    "                if 'learning_rate' in params:\n",
    "                    init_params['learning_rate'] = params.pop('learning_rate')\n",
    "                if 'epochs' in params:\n",
    "                    init_params['epochs'] = params.pop('epochs')\n",
    "                if 'angle_loss_weight' in params:\n",
    "                    init_params['angle_loss_weight'] = params.pop('angle_loss_weight')\n",
    "            \n",
    "            # Handle training parameters that go into init for BiPO\n",
    "            if config.name.startswith(\"BiPO\"):\n",
    "                if 'beta' in params:\n",
    "                    init_params['beta'] = params.pop('beta')\n",
    "                if 'learning_rate' in params:\n",
    "                    init_params['learning_rate'] = params.pop('learning_rate')\n",
    "                if 'num_epochs' in params:\n",
    "                    init_params['num_epochs'] = params.pop('num_epochs')\n",
    "            \n",
    "            # Handle training parameters that go into init for KSteering\n",
    "            if config.name.startswith(\"KSteering\"):\n",
    "                if 'alpha' in params:\n",
    "                    init_params['alpha'] = params.pop('alpha')\n",
    "                if 'num_epochs' in params:\n",
    "                    init_params['num_epochs'] = params.pop('num_epochs')\n",
    "            \n",
    "            # Create and train the method\n",
    "            method = config.method_class(**init_params)\n",
    "            \n",
    "            # Special handling for different methods\n",
    "            if isinstance(method, DAC):\n",
    "                method.set_model_reference(hf_model)\n",
    "            \n",
    "            # Train on the appropriate layer's data\n",
    "            if layer in layer_pair_sets:\n",
    "                method.train(layer_pair_sets[layer], layer)\n",
    "            else:\n",
    "                print(f\"Warning: No training data for layer {layer}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate steered responses and evaluate on all test prompts\n",
    "            prompt_scores = []\n",
    "            prompt_responses = {}\n",
    "            \n",
    "            for prompt in TEST_PROMPTS:\n",
    "                steered_response = generate_with_steering(prompt, method, layer, **params)\n",
    "                prompt_responses[prompt] = steered_response\n",
    "                \n",
    "                # Evaluate\n",
    "                scores = evaluator.evaluate_response(\n",
    "                    prompt, unsteered_responses[prompt], steered_response, TRAIT_NAME\n",
    "                )\n",
    "                prompt_scores.append(scores['overall'])\n",
    "            \n",
    "            # Average score across all prompts\n",
    "            avg_score = sum(prompt_scores) / len(prompt_scores)\n",
    "            \n",
    "            result = {\n",
    "                'params': {**params, 'layer': layer, **{k: v for k, v in init_params.items() if k not in ['device', 'target_labels', 'avoid_labels']}},\n",
    "                'avg_score': avg_score,\n",
    "                'prompt_scores': prompt_scores,\n",
    "                'responses': prompt_responses\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Update best if needed\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = result['params']\n",
    "                best_responses = prompt_responses\n",
    "            \n",
    "            # Progress update - more frequent for large searches\n",
    "            update_interval = 10 if len(all_combinations) < 1000 else 100\n",
    "            if (i + 1) % update_interval == 0:\n",
    "                print(f\"  Progress: {i+1}/{len(all_combinations)} ({(i+1)/len(all_combinations)*100:.1f}%) - Best avg score so far: {best_score:.1f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with params {params}: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nBest parameters for {config.name}:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"Best average score: {best_score:.1f}/10\")\n",
    "    print(f\"\\nBest responses for each prompt:\")\n",
    "    for prompt, response in best_responses.items():\n",
    "        print(f\"  {prompt}: {response[:50]}...\")\n",
    "    \n",
    "    return {\n",
    "        'config_name': config.name,\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'best_responses': best_responses,\n",
    "        'all_results': all_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations to test for ALL methods (matching CLI):\n",
      "  CAA: 160 combinations\n",
      "  CAA_L2: 160 combinations\n",
      "  HPR: 4,320 combinations\n",
      "  HPR_Beta0.5: 1,440 combinations\n",
      "  BiPO: 5,760 combinations\n",
      "  BiPO_Beta0.05: 1,440 combinations\n",
      "  KSteering: 1,440 combinations\n",
      "  KSteering_Alpha3: 480 combinations\n",
      "  DAC: 192 combinations\n",
      "\n",
      "TOTAL: 15,392 combinations across all methods\n",
      "\n",
      "Warning: This will take many hours to complete!\n",
      "\n",
      "============================================================\n",
      "OPTIMIZING CAA\n",
      "============================================================\n",
      "Testing ALL 160 parameter combinations...\n",
      "  Progress: 10/160 (6.2%) - Best avg score so far: 5.4\n",
      "  Progress: 20/160 (12.5%) - Best avg score so far: 5.4\n",
      "  Progress: 30/160 (18.8%) - Best avg score so far: 5.4\n",
      "  Progress: 40/160 (25.0%) - Best avg score so far: 5.8\n",
      "  Progress: 50/160 (31.2%) - Best avg score so far: 5.8\n",
      "  Progress: 60/160 (37.5%) - Best avg score so far: 5.8\n",
      "  Progress: 70/160 (43.8%) - Best avg score so far: 5.8\n",
      "  Progress: 80/160 (50.0%) - Best avg score so far: 5.8\n",
      "  Progress: 90/160 (56.2%) - Best avg score so far: 5.8\n",
      "  Progress: 100/160 (62.5%) - Best avg score so far: 5.8\n",
      "  Progress: 110/160 (68.8%) - Best avg score so far: 5.8\n",
      "  Progress: 120/160 (75.0%) - Best avg score so far: 5.8\n",
      "  Progress: 130/160 (81.2%) - Best avg score so far: 5.8\n",
      "  Progress: 140/160 (87.5%) - Best avg score so far: 5.8\n",
      "  Progress: 150/160 (93.8%) - Best avg score so far: 5.8\n",
      "  Progress: 160/160 (100.0%) - Best avg score so far: 5.8\n",
      "\n",
      "Best parameters for CAA:\n",
      "  strength: 1.0\n",
      "  layer: 1\n",
      "Best average score: 5.8/10\n",
      "\n",
      "Best responses for each prompt:\n",
      "  What should I do today?: I want to relax and unwind, but I also want to mak...\n",
      "  Tell me about your thoughts on life.: What's your perspective on the human experience?\n",
      "M...\n",
      "  How would you handle a difficult situation?: (e.g. a coworker is being uncooperative, a custome...\n",
      "  What's your opinion on modern technology?: Do you think it has made our lives better or worse...\n",
      "  Describe your ideal weekend.: I donâ€™t know about you, but I am a big fan of a re...\n",
      "\n",
      "============================================================\n",
      "OPTIMIZING CAA_L2\n",
      "============================================================\n",
      "Testing ALL 160 parameter combinations...\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 0.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 1.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.0}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "  Error with params {'strength': 2.5}: 'VectorNormalizer' object has no attribute 'normalize_l2_unit'\n",
      "\n",
      "Best parameters for CAA_L2:\n",
      "Best average score: -1.0/10\n",
      "\n",
      "Best responses for each prompt:\n",
      "\n",
      "============================================================\n",
      "OPTIMIZING HPR\n",
      "============================================================\n",
      "Testing ALL 4320 parameter combinations...\n",
      "  Progress: 100/4320 (2.3%) - Best avg score so far: 5.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configs_to_test:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_steering_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# No limit - test ALL combinations\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         optimization_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[24], line 78\u001b[0m, in \u001b[0;36moptimize_steering_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     75\u001b[0m     prompt_responses[prompt] \u001b[38;5;241m=\u001b[39m steered_response\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsteered_responses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteered_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIT_NAME\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     prompt_scores\u001b[38;5;241m.\u001b[39mappend(scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Average score across all prompts\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CodingProjects/Wisent/wisent-activation-guardrails/examples/evaluate_personal.py:50\u001b[0m, in \u001b[0;36mSteeringEvaluator.evaluate_response\u001b[0;34m(self, prompt, unsteered_response, steered_response, trait_description)\u001b[0m\n\u001b[1;32m     47\u001b[0m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifference\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_difference(unsteered_response, steered_response)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 2. Evaluate coherence\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteered_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 3. Evaluate trait alignment\u001b[39;00m\n\u001b[1;32m     53\u001b[0m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrait_alignment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_trait_alignment(\n\u001b[1;32m     54\u001b[0m     prompt, steered_response, trait_description\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/CodingProjects/Wisent/wisent-activation-guardrails/examples/evaluate_personal.py:90\u001b[0m, in \u001b[0;36mSteeringEvaluator._evaluate_coherence\u001b[0;34m(self, prompt, response)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the coherence and quality of the response.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mRate the coherence and quality of this response on a scale of 0-10 (0 = incoherent/nonsensical, 10 = perfectly coherent and well-written):\u001b[39m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124mProvide only a number between 0 and 10:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 90\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_numeric_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/Documents/CodingProjects/Wisent/wisent-activation-guardrails/examples/evaluate_personal.py:113\u001b[0m, in \u001b[0;36mSteeringEvaluator._get_numeric_score\u001b[0;34m(self, eval_prompt)\u001b[0m\n\u001b[1;32m    110\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(eval_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Low temperature for consistent scoring\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Extract numeric score\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/generation/utils.py:2625\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2618\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2619\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2620\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2622\u001b[0m     )\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2625\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2636\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2637\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2638\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2639\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2640\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2642\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/generation/utils.py:3609\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3607\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3609\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3612\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3613\u001b[0m     outputs,\n\u001b[1;32m   3614\u001b[0m     model_kwargs,\n\u001b[1;32m   3615\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3616\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:569\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m    568\u001b[0m slice_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m-\u001b[39mlogits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[0;32m--> 569\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimize ALL methods matching the CLI implementation\n",
    "# This will test all 9 method configurations used by the Wisent-Guard CLI\n",
    "configs_to_test = [\n",
    "    steering_configs[0],  # CAA - 160 combinations\n",
    "    steering_configs[1],  # CAA_L2 - 160 combinations\n",
    "    steering_configs[2],  # HPR - 4,320 combinations\n",
    "    steering_configs[3],  # HPR_Beta0.5 - 2,880 combinations\n",
    "    steering_configs[4],  # BiPO - 3,600 combinations\n",
    "    steering_configs[5],  # BiPO_Beta0.05 - 2,880 combinations\n",
    "    steering_configs[6],  # KSteering - 2,880 combinations\n",
    "    steering_configs[7],  # KSteering_Alpha3 - 2,880 combinations\n",
    "    steering_configs[8],  # DAC - 192 combinations\n",
    "]\n",
    "\n",
    "print(\"Total combinations to test for ALL methods (matching CLI):\")\n",
    "total_all = 0\n",
    "for config in configs_to_test:\n",
    "    param_names = list(config.steering_params.keys())\n",
    "    param_values = list(config.steering_params.values())\n",
    "    total_combos = 1\n",
    "    for values in param_values:\n",
    "        total_combos *= len(values)\n",
    "    print(f\"  {config.name}: {total_combos:,} combinations\")\n",
    "    total_all += total_combos\n",
    "print(f\"\\nTOTAL: {total_all:,} combinations across all methods\")\n",
    "print(\"\\nWarning: This will take many hours to complete!\")\n",
    "\n",
    "optimization_results = []\n",
    "for config in configs_to_test:\n",
    "    try:\n",
    "        result = optimize_steering_config(config)  # No limit - test ALL combinations\n",
    "        optimization_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to optimize {config.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which layers work best across methods\n",
    "layer_effectiveness = {}\n",
    "for layer in LAYER_RANGE:\n",
    "    layer_effectiveness[layer] = []\n",
    "\n",
    "for result in optimization_results:\n",
    "    for test_result in result['all_results']:\n",
    "        layer = test_result['params']['layer']\n",
    "        score = test_result['avg_score']\n",
    "        layer_effectiveness[layer].append(score)\n",
    "\n",
    "# Calculate average effectiveness per layer\n",
    "print(\"\\nLayer Effectiveness Analysis:\")\n",
    "print(\"=\"*40)\n",
    "for layer, scores in layer_effectiveness.items():\n",
    "    if scores:\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Layer {layer}: {avg_score:.2f} (based on {len(scores)} tests)\")\n",
    "\n",
    "# Plot layer effectiveness\n",
    "plt.figure(figsize=(10, 6))\n",
    "layers = list(layer_effectiveness.keys())\n",
    "avg_scores = [sum(scores)/len(scores) if scores else 0 for scores in layer_effectiveness.values()]\n",
    "plt.bar(layers, avg_scores)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Average Effectiveness Score')\n",
    "plt.title(f'Steering Effectiveness by Layer for \"{TRAIT_NAME}\" trait')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter sensitivity for each method\n",
    "def analyze_parameter_sensitivity(optimization_result):\n",
    "    \"\"\"Analyze how sensitive the score is to each parameter.\"\"\"\n",
    "    config_name = optimization_result['config_name']\n",
    "    all_results = optimization_result['all_results']\n",
    "    \n",
    "    if not all_results:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nParameter Sensitivity for {config_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get all parameter names (excluding layer for now)\n",
    "    param_names = [k for k in all_results[0]['params'].keys() if k != 'layer']\n",
    "    \n",
    "    for param_name in param_names:\n",
    "        # Group results by parameter value\n",
    "        param_groups = {}\n",
    "        for result in all_results:\n",
    "            param_val = result['params'].get(param_name)\n",
    "            if param_val is not None:\n",
    "                if param_val not in param_groups:\n",
    "                    param_groups[param_val] = []\n",
    "                param_groups[param_val].append(result['avg_score'])\n",
    "        \n",
    "        # Calculate variance across parameter values\n",
    "        if len(param_groups) > 1:\n",
    "            group_means = {val: sum(scores)/len(scores) for val, scores in param_groups.items()}\n",
    "            variance = np.var(list(group_means.values()))\n",
    "            \n",
    "            print(f\"{param_name}:\")\n",
    "            for val, mean_score in sorted(group_means.items()):\n",
    "                print(f\"  {val}: {mean_score:.2f}\")\n",
    "            print(f\"  Variance: {variance:.3f}\")\n",
    "\n",
    "# Run sensitivity analysis\n",
    "for result in optimization_results:\n",
    "    analyze_parameter_sensitivity(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "Let's compare the optimal parameters and performance across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison of all optimized methods\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON - OPTIMAL CONFIGURATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort methods by best score\n",
    "sorted_results = sorted(optimization_results, key=lambda x: x['best_score'], reverse=True)\n",
    "\n",
    "print(f\"\\nMethod Ranking by Overall Score for '{TRAIT_NAME}' trait:\")\n",
    "print(\"-\" * 40)\n",
    "for i, result in enumerate(sorted_results, 1):\n",
    "    print(f\"\\n{i}. {result['config_name']}:\")\n",
    "    print(f\"   Best average score: {result['best_score']:.1f}/10\")\n",
    "    print(f\"   Optimal parameters:\")\n",
    "    for param, value in result['best_params'].items():\n",
    "        print(f\"     - {param}: {value}\")\n",
    "    print(f\"   Example responses:\")\n",
    "    for j, (prompt, response) in enumerate(list(result['best_responses'].items())[:2]):\n",
    "        print(f\"     Prompt {j+1}: {prompt}\")\n",
    "        print(f\"     Response: {response[:60]}...\")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\\nSummary Table:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<20} {'Avg Score':<12} {'Layer':<10} {'Strength/Alpha':<15} {'Other Params':<20}\")\n",
    "print(\"-\" * 80)\n",
    "for result in sorted_results:\n",
    "    method = result['config_name']\n",
    "    score = result['best_score']\n",
    "    layer = result['best_params'].get('layer', 'N/A')\n",
    "    strength = result['best_params'].get('strength', result['best_params'].get('alpha', 'N/A'))\n",
    "    \n",
    "    # Collect other important params\n",
    "    other_params = []\n",
    "    for k, v in result['best_params'].items():\n",
    "        if k not in ['layer', 'strength', 'alpha']:\n",
    "            other_params.append(f\"{k}={v}\")\n",
    "    other_str = ', '.join(other_params[:2]) if other_params else 'N/A'\n",
    "    \n",
    "    print(f\"{method:<20} {score:<12.1f} {layer:<10} {strength:<15.2f} {other_str:<20}\")\n",
    "\n",
    "# Save optimization results to JSON\n",
    "optimization_data = {\n",
    "    \"trait\": TRAIT_NAME,\n",
    "    \"test_prompts\": TEST_PROMPTS,\n",
    "    \"unsteered_responses\": unsteered_responses,\n",
    "    \"optimization_results\": []\n",
    "}\n",
    "\n",
    "for result in sorted_results:\n",
    "    optimization_data[\"optimization_results\"].append({\n",
    "        \"method\": result['config_name'],\n",
    "        \"best_score\": result['best_score'],\n",
    "        \"best_params\": result['best_params'],\n",
    "        \"best_responses\": result['best_responses']\n",
    "    })\n",
    "\n",
    "results_filename = f\"optimization_results_{TRAIT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(results_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(optimization_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved optimization results to {results_filename}\")\n",
    "\n",
    "print(\"\\nâœ… Comprehensive optimization complete!\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Different normalization methods (L2, cross-behavior) can significantly affect performance\")\n",
    "print(\"2. Optimal layer varies by method - not always the classification layer\")\n",
    "print(\"3. Strength/alpha parameters need careful tuning for each method\")\n",
    "print(\"4. HPR and BiPO have additional training hyperparameters that affect performance\")\n",
    "print(\"5. Evaluation across multiple prompts provides more robust optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
