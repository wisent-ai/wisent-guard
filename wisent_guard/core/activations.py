import torch
import torch.nn.functional as F
from .layer import Layer
from enum import Enum
from typing import Optional, Dict, Any, Union, List, Tuple
try:
    from .contrastive_pairs import ContrastivePair
except ImportError:
    from contrastive_pairs import ContrastivePair
from .response import PositiveResponse, NegativeResponse

class ActivationAggregationMethod(Enum):
    LAST_TOKEN = "last_token"
    MEAN = "mean"
    MAX = "max"

class Activations:
    def __init__(self, tensor, layer, aggregation_method=None):
        self.tensor = tensor
        self.layer = layer
        self.aggregation_method = aggregation_method or ActivationAggregationMethod.LAST_TOKEN
    
    def get_aggregated(self):
        """
        Get aggregated activations based on the aggregation method.
        
        Returns:
            torch.Tensor: Aggregated activation tensor
        """
        if self.aggregation_method == ActivationAggregationMethod.LAST_TOKEN:
            # Return the last token's activations
            if len(self.tensor.shape) >= 3:  # [batch, seq_len, hidden_dim]
                return self.tensor[0, -1, :]  # Take last token of first batch
            elif len(self.tensor.shape) == 2:  # [seq_len, hidden_dim]
                return self.tensor[-1, :]  # Take last token
            else:
                return self.tensor  # Already aggregated
                
        elif self.aggregation_method == ActivationAggregationMethod.MEAN:
            # Return mean across sequence dimension
            if len(self.tensor.shape) >= 3:  # [batch, seq_len, hidden_dim]
                return self.tensor[0].mean(dim=0)  # Mean across sequence length
            elif len(self.tensor.shape) == 2:  # [seq_len, hidden_dim]
                return self.tensor.mean(dim=0)  # Mean across sequence length
            else:
                return self.tensor
                
        elif self.aggregation_method == ActivationAggregationMethod.MAX:
            # Return max across sequence dimension
            if len(self.tensor.shape) >= 3:  # [batch, seq_len, hidden_dim]
                return self.tensor[0].max(dim=0)[0]  # Max across sequence length
            elif len(self.tensor.shape) == 2:  # [seq_len, hidden_dim]
                return self.tensor.max(dim=0)[0]  # Max across sequence length
            else:
                return self.tensor
        else:
            # Default to last token
            if len(self.tensor.shape) >= 3:  # [batch, seq_len, hidden_dim]
                return self.tensor[0, -1, :]  # Take last token of first batch
            elif len(self.tensor.shape) == 2:  # [seq_len, hidden_dim]
                return self.tensor[-1, :]  # Take last token
            else:
                return self.tensor  # Already aggregated

    def calculate_similarity(self, other_tensor: torch.Tensor, method: str = "cosine") -> float:
        """
        Calculate similarity between this activation and another tensor.
        
        Args:
            other_tensor: Tensor to compare against (e.g., contrastive vector)
            method: Similarity method ("cosine", "dot", "euclidean")
            
        Returns:
            Similarity score
        """
        # Get aggregated activation
        activation = self.get_aggregated()
        
        # Ensure tensors are on the same device and have compatible shapes
        if activation.device != other_tensor.device:
            other_tensor = other_tensor.to(activation.device)
        
        # Flatten tensors if needed
        if len(activation.shape) > 1:
            activation = activation.flatten()
        if len(other_tensor.shape) > 1:
            other_tensor = other_tensor.flatten()
        
        # Handle dimension mismatch
        if activation.shape[0] != other_tensor.shape[0]:
            min_dim = min(activation.shape[0], other_tensor.shape[0])
            activation = activation[:min_dim]
            other_tensor = other_tensor[:min_dim]
        
        try:
            if method == "cosine":
                # Cosine similarity
                cos_sim = F.cosine_similarity(
                    activation.unsqueeze(0), 
                    other_tensor.unsqueeze(0), 
                    dim=1
                )
                # Convert to similarity score (0 to 1)
                similarity = (cos_sim.item() + 1.0) / 2.0
                return max(0.0, min(1.0, similarity))
                
            elif method == "dot":
                # Dot product similarity
                dot_product = torch.dot(activation, other_tensor)
                return dot_product.item()
                
            elif method == "euclidean":
                # Negative euclidean distance (higher = more similar)
                distance = torch.norm(activation - other_tensor)
                return -distance.item()
                
            else:
                raise ValueError(f"Unknown similarity method: {method}")
                
        except Exception as e:
            # Return 0 similarity on error
            return 0.0

    def compare_with_vectors(self, vector_dict: Dict[str, torch.Tensor], threshold: float = 0.7) -> Dict[str, Any]:
        """
        Compare this activation with multiple contrastive vectors.
        
        Args:
            vector_dict: Dictionary mapping category names to contrastive vectors
            threshold: Threshold for determining harmful content
            
        Returns:
            Dictionary with comparison results for each category
        """
        results = {}
        
        for category, vector in vector_dict.items():
            similarity = self.calculate_similarity(vector)
            is_harmful = similarity >= threshold
            
            results[category] = {
                "similarity": similarity,
                "is_harmful": is_harmful,
                "threshold": threshold
            }
        
        return results

    def extract_features_for_classifier(self) -> torch.Tensor:
        """
        Extract features suitable for classifier input.
        
        Returns:
            Flattened tensor ready for classification
        """
        features = self.get_aggregated()
        
        # Ensure it's a PyTorch tensor
        if not isinstance(features, torch.Tensor):
            features = torch.tensor(features, dtype=torch.float32)
        
        # Flatten if needed
        if len(features.shape) > 1:
            features = features.flatten()
        
        return features

    def to_device(self, device: str) -> 'Activations':
        """
        Move activations to a specific device.
        
        Args:
            device: Target device (e.g., 'cuda', 'cpu', 'mps')
            
        Returns:
            New Activations object on the target device
        """
        new_tensor = self.tensor.to(device)
        return Activations(
            tensor=new_tensor,
            layer=self.layer,
            aggregation_method=self.aggregation_method
        )

    def normalize(self) -> 'Activations':
        """
        Normalize the activation tensor.
        
        Returns:
            New Activations object with normalized tensor
        """
        aggregated = self.get_aggregated()
        
        # L2 normalization
        norm = torch.norm(aggregated, p=2, dim=-1, keepdim=True)
        normalized = aggregated / (norm + 1e-8)  # Add small epsilon to avoid division by zero
        
        return Activations(
            tensor=normalized,
            layer=self.layer,
            aggregation_method=self.aggregation_method
        )

    def get_statistics(self) -> Dict[str, float]:
        """
        Get statistical information about the activations.
        
        Returns:
            Dictionary with statistics
        """
        aggregated = self.get_aggregated()
        
        return {
            "mean": float(aggregated.mean()),
            "std": float(aggregated.std()),
            "min": float(aggregated.min()),
            "max": float(aggregated.max()),
            "norm": float(torch.norm(aggregated)),
            "shape": list(aggregated.shape),
            "device": str(aggregated.device),
            "dtype": str(aggregated.dtype)
        }

    @classmethod
    def from_model_output(cls, model_outputs, layer: Layer, aggregation_method=None) -> 'Activations':
        """
        Create Activations object from model forward pass outputs.
        
        Args:
            model_outputs: Output from model forward pass
            layer: Layer object specifying which layer to extract
            aggregation_method: How to aggregate the activations
            
        Returns:
            Activations object
        """
        if hasattr(model_outputs, 'hidden_states') and model_outputs.hidden_states is not None:
            hidden_states = model_outputs.hidden_states
            
            # Get the hidden state for the specified layer (add 1 because hidden_states[0] is embeddings)
            if layer.index + 1 < len(hidden_states):
                layer_hidden_state = hidden_states[layer.index + 1]
                
                return cls(
                    tensor=layer_hidden_state,
                    layer=layer,
                    aggregation_method=aggregation_method
                )
            else:
                raise ValueError(f"Layer {layer.index} not found in model with {len(hidden_states)} layers")
        else:
            raise ValueError("Model outputs don't contain hidden_states")

    @classmethod
    def from_tensor_dict(cls, tensor_dict: Dict[str, torch.Tensor], layer: Layer, aggregation_method=None) -> 'Activations':
        """
        Create Activations object from a dictionary of tensors (legacy compatibility).
        
        Args:
            tensor_dict: Dictionary containing activation tensors
            layer: Layer object
            aggregation_method: How to aggregate the activations
            
        Returns:
            Activations object
        """
        # Try to find the activation tensor in the dictionary
        if 'activations' in tensor_dict:
            tensor = tensor_dict['activations']
        elif str(layer.index) in tensor_dict:
            tensor = tensor_dict[str(layer.index)]
        elif layer.index in tensor_dict:
            tensor = tensor_dict[layer.index]
        else:
            raise ValueError(f"No activation tensor found for layer {layer.index} in dictionary")
        
        return cls(
            tensor=tensor,
            layer=layer,
            aggregation_method=aggregation_method
        )

    def __repr__(self) -> str:
        """String representation of the Activations object."""
        aggregated = self.get_aggregated()
        return f"Activations(layer={self.layer.index}, shape={list(aggregated.shape)}, method={self.aggregation_method.value})"

    # Monitoring functionality
    def cosine_similarity(self, other: 'Activations') -> float:
        """Calculate cosine similarity with another activation."""
        return self.calculate_similarity(other.get_aggregated(), method="cosine")
    
    def dot_product_similarity(self, other: 'Activations') -> float:
        """Calculate dot product similarity with another activation."""
        return self.calculate_similarity(other.get_aggregated(), method="dot")
    
    def euclidean_distance(self, other: 'Activations') -> float:
        """Calculate euclidean distance with another activation."""
        return -self.calculate_similarity(other.get_aggregated(), method="euclidean")

class ActivationMonitor:
    """
    Monitor for tracking and analyzing model activations.
    Integrated into the activations primitive.
    """
    
    def __init__(self):
        """Initialize activation monitor."""
        self.current_activations = {}
        self.activation_history = []
    
    def store_activations(
        self,
        activations: Dict[int, Activations],
        text: Optional[str] = None
    ) -> None:
        """Store activations with optional text context."""
        self.current_activations = activations
        self.activation_history.append({
            "text": text,
            "activations": activations
        })
    
    def analyze_activations(
        self,
        activations: Optional[Dict[int, Activations]] = None
    ) -> Dict[int, Dict[str, Any]]:
        """Analyze activations for patterns and statistics."""
        if activations is None:
            activations = self.current_activations
        
        analysis = {}
        for layer_idx, activation in activations.items():
            stats = activation.get_statistics()
            features = activation.extract_features_for_classifier()
            
            analysis[layer_idx] = {
                "statistics": stats,
                "feature_vector": features,
                "tensor_shape": activation.tensor.shape,
                "device": str(activation.tensor.device)
            }
        
        return analysis
    
    def compare_with_baseline(
        self,
        baseline_activations: Dict[int, Activations],
        current_activations: Optional[Dict[int, Activations]] = None
    ) -> Dict[int, Dict[str, float]]:
        """Compare current activations with baseline."""
        if current_activations is None:
            current_activations = self.current_activations
        
        comparisons = {}
        for layer_idx in current_activations:
            if layer_idx in baseline_activations:
                current = current_activations[layer_idx]
                baseline = baseline_activations[layer_idx]
                
                comparisons[layer_idx] = {
                    "cosine_similarity": current.cosine_similarity(baseline),
                    "dot_product": current.dot_product_similarity(baseline),
                    "euclidean_distance": current.euclidean_distance(baseline)
                }
        
        return comparisons
    
    def detect_anomalies(
        self,
        threshold: float = 0.8,
        activations: Optional[Dict[int, Activations]] = None
    ) -> Dict[int, bool]:
        """Detect anomalies in activations based on historical patterns."""
        if activations is None:
            activations = self.current_activations
        
        if len(self.activation_history) < 2:
            return {layer_idx: False for layer_idx in activations}
        
        anomalies = {}
        for layer_idx in activations:
            historical_activations = []
            for entry in self.activation_history[:-1]:
                if layer_idx in entry["activations"]:
                    historical_activations.append(entry["activations"][layer_idx])
            
            if not historical_activations:
                anomalies[layer_idx] = False
                continue
            
            current = activations[layer_idx]
            similarities = [current.cosine_similarity(hist) for hist in historical_activations]
            avg_similarity = sum(similarities) / len(similarities)
            anomalies[layer_idx] = avg_similarity < threshold
        
        return anomalies
    
    def clear_history(self) -> None:
        """Clear activation history."""
        self.activation_history.clear()
    
    def save_activations(self, filepath: str) -> None:
        """Save current activations to file."""
        import torch
        
        if not self.current_activations:
            raise ValueError("No activations to save")
        
        save_data = {}
        for layer_idx, activation in self.current_activations.items():
            save_data[layer_idx] = activation.tensor
        
        torch.save(save_data, filepath)
    
    def load_activations(self, filepath: str) -> Dict[int, Activations]:
        """Load activations from file."""
        import torch
        from .layer import Layer
        
        loaded_data = torch.load(filepath)
        activations = {}
        for layer_idx, tensor in loaded_data.items():
            layer_obj = Layer(index=layer_idx, type="transformer")
            activations[layer_idx] = Activations(tensor=tensor, layer=layer_obj)
        
        self.current_activations = activations
        return activations

class TestActivationCache:
    """
    Cache for test-time activations with associated questions and responses.
    Allows saving activations from one layer and reusing them for different layer classifiers.
    """
    
    def __init__(self):
        self.activations = []  # List of dicts with 'question', 'response', 'activations', 'layer'
    
    def add_activation(self, question: str, response: str, activations: 'Activations', layer: int):
        """Add a test activation to the cache."""
        self.activations.append({
            'question': question,
            'response': response,  
            'activations': activations,
            'layer': layer
        })
    
    def save_to_file(self, filepath: str) -> None:
        """Save cached activations to file."""
        import torch
        import json
        
        if not self.activations:
            raise ValueError("No activations to save")
        
        # Prepare data for saving
        save_data = {
            'metadata': {
                'num_samples': len(self.activations),
                'layers': list(set(item['layer'] for item in self.activations)),
                'created_at': __import__('datetime').datetime.now().isoformat()
            },
            'activations': []
        }
        
        for i, item in enumerate(self.activations):
            activation_data = {
                'index': i,
                'question': item['question'],
                'response': item['response'],
                'layer': item['layer'],
                'activation_tensor': item['activations'].tensor,
                'activation_shape': list(item['activations'].tensor.shape),
                'aggregation_method': item['activations'].aggregation_method.value if hasattr(item['activations'].aggregation_method, 'value') else str(item['activations'].aggregation_method)
            }
            save_data['activations'].append(activation_data)
        
        # Save using torch
        torch.save(save_data, filepath)
        print(f"ðŸ’¾ Saved {len(self.activations)} test activations to {filepath}")
    
    @classmethod
    def load_from_file(cls, filepath: str) -> 'TestActivationCache':
        """Load cached activations from file."""
        import torch
        from .layer import Layer
        
        loaded_data = torch.load(filepath, map_location='cpu')
        cache = cls()
        
        metadata = loaded_data.get('metadata', {})
        print(f"ðŸ“ Loading {metadata.get('num_samples', 0)} test activations from {filepath}")
        
        for item in loaded_data['activations']:
            # Reconstruct Activations object
            layer_obj = Layer(index=item['layer'], type="transformer")
            
            # Reconstruct aggregation method
            from .activations import ActivationAggregationMethod
            agg_method = ActivationAggregationMethod.LAST_TOKEN  # default
            try:
                if item.get('aggregation_method'):
                    agg_method = ActivationAggregationMethod(item['aggregation_method'])
            except:
                pass
            
            activations = Activations(
                tensor=item['activation_tensor'],
                layer=layer_obj,
                aggregation_method=agg_method
            )
            
            cache.add_activation(
                question=item['question'],
                response=item['response'],
                activations=activations,
                layer=item['layer']
            )
        
        return cache
    
    def get_activations_for_layer(self, target_layer: int) -> List[Dict]:
        """Get all activations for a specific layer."""
        return [item for item in self.activations if item['layer'] == target_layer]
    
    def __len__(self):
        return len(self.activations)
    
    def __repr__(self):
        layers = list(set(item['layer'] for item in self.activations))
        return f"TestActivationCache({len(self.activations)} samples, layers: {layers})"


