{
  "arc_challenge": "log-likelihoods",
  "arc_easy": "log-likelihoods",
  "asdiv": "text-generation",
  "boolq": "log-likelihoods",
  "cb": "log-likelihoods",
  "copa": "log-likelihoods",
  "coqa": "text-generation",
  "drop": "text-generation",
  "gsm8k": "text-generation",
  "math": "text-generation",
  "math500": "text-generation",
  "hendrycks_math": "text-generation",
  "aime": "text-generation",
  "aime2025": "text-generation",
  "aime2024": "text-generation",
  "hmmt": "text-generation",
  "hmmt_feb_2025": "text-generation",
  "polymath": "text-generation",
  "polymath_en_medium": "text-generation",
  "polymath_zh_medium": "text-generation",
  "polymath_en_high": "text-generation",
  "polymath_zh_high": "text-generation",
  "livemathbench": "text-generation",
  "livemathbench_cnmo_en": "text-generation",
  "livemathbench_cnmo_zh": "text-generation",
  "hellaswag": "log-likelihoods",
  "logiqa": "log-likelihoods",
  "logiqa2": "log-likelihoods",
  "math_qa": "text-generation",
  "mmlu": "log-likelihoods",
  "mmmlu": "log-likelihoods",
  "gpqa_main_zeroshot": "log-likelihoods",
  "gpqa_main_cot_zeroshot": "text-generation",
  "gpqa_diamond_zeroshot": "log-likelihoods",
  "gpqa_diamond_cot_zeroshot": "text-generation",
  "gpqa_extended_zeroshot": "log-likelihoods",
  "gpqa_extended_cot_zeroshot": "text-generation",

  "leaderboard_gpqa": "log-likelihoods",
  "leaderboard_gpqa_main": "log-likelihoods",
  "leaderboard_gpqa_diamond": "log-likelihoods",
  "leaderboard_gpqa_extended": "log-likelihoods",

  "supergpqa": "log-likelihoods",
  "supergpqa_biology": "log-likelihoods",
  "supergpqa_chemistry": "log-likelihoods",
  "supergpqa_physics": "log-likelihoods",

  "multirc": "log-likelihoods",
  "qa4mre_2013": "log-likelihoods",
  "mutual": "log-likelihoods",
  "naturalqs": "text-generation",
  "openbookqa": "log-likelihoods",
  "piqa": "log-likelihoods",
  "prost": "log-likelihoods",
  "pubmedqa": "log-likelihoods",
  "race": "log-likelihoods",
  "record": "text-generation",
  "sciq": "log-likelihoods",
  "squad2": "text-generation",
  "swag": "log-likelihoods",
  "toxigen": "log-likelihoods",
  "triviaqa": "text-generation",
  "truthfulqa_gen": "text-generation",
  "truthfulqa_mc1": "log-likelihoods",
  "truthfulqa_mc2": "log-likelihoods",
  "webqs": "text-generation",
  "wic": "log-likelihoods",
  "wikitext": "perplexity",
  "winogrande": "log-likelihoods",
  "wsc": "log-likelihoods",
  "wsc273": "log-likelihoods",
  "mc-taco": "log-likelihoods",
  "social_iqa": "log-likelihoods",
  "humaneval": "code-execution",
  "humaneval_plus": "code-execution",
  "instructhumaneval": "code-execution",
  "apps": "code-execution",
  "mbpp": "code-execution",
  "mbpp_plus": "code-execution",
  "ds1000": "code-execution",
  "humanevalpack": "code-execution",
  "multiple_py": "code-execution",
  "multiple_js": "code-execution",
  "multiple_java": "code-execution",
  "multiple_cpp": "code-execution",
  "multiple_rs": "code-execution",
  "multiple_go": "code-execution",
  "recode": "code-execution",
  "conala": "code-execution",
  "concode": "code-execution",
  "mercury": "code-execution",
  "codexglue_code_to_text_python": "text-generation",
  "codexglue_code_to_text_go": "text-generation",
  "codexglue_code_to_text_java": "text-generation",
  "codexglue_code_to_text_javascript": "text-generation",
  "codexglue_code_to_text_php": "text-generation",
  "codexglue_code_to_text_ruby": "text-generation",
  "livecodebench": "code-execution",
  "hle": "text-generation",
  "hle_exact_match": "text-generation",
  "hle_multiple_choice": "log-likelihoods"
}