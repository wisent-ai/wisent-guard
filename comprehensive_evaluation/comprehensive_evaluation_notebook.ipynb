{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive Evaluation Framework for Wisent Guard\n",
    "\n",
    "This notebook provides an interactive interface for running comprehensive evaluations that properly separate:\n",
    "\n",
    "1. **üéØ Benchmark Performance**: How well the model solves mathematical problems\n",
    "2. **üîç Probe Performance**: How well probes detect correctness from model activations\n",
    "3. **‚öôÔ∏è DAC Hyperparameter Optimization**: Grid search to find optimal DAC configurations\n",
    "\n",
    "## Key Features:\n",
    "- **Real Data Integration**: Uses GSM8KExtractor to get contrastive pairs from training data\n",
    "- **DAC Hyperparameter Grid Search**: Systematic optimization of entropy_threshold, ptop, and max_alpha\n",
    "- **Real-time Progress**: Live updates during evaluation with tqdm\n",
    "- **Rich Visualizations**: Comprehensive plots and analysis\n",
    "- **Modular Design**: Clean separation of concerns\n",
    "- **Export Results**: Save results and generate reports\n",
    "\n",
    "## DAC Hyperparameters:\n",
    "- **entropy_threshold**: Controls dynamic steering based on entropy (default: 1.0)\n",
    "- **ptop**: Probability threshold for KL-based dynamic control (default: 0.4)\n",
    "- **max_alpha**: Maximum steering intensity (default: 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wandb connection cleaned up\n",
      "‚úÖ All imports successful!\n",
      "üìç Working directory: /workspace/wisent-guard/comprehensive_evaluation\n",
      "üêç Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "üíæ HuggingFace cache: /workspace/.cache/huggingface\n",
      "üîó Wandb status: ‚úÖ Ready\n",
      "\n",
      "‚úÖ Logged into HuggingFace as: jfpio\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set HuggingFace cache to permanent directory\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/.cache/huggingface/datasets'\n",
    "\n",
    "# Create cache directories if they don't exist\n",
    "os.makedirs('/workspace/.cache/huggingface/transformers', exist_ok=True)\n",
    "os.makedirs('/workspace/.cache/huggingface/datasets', exist_ok=True)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = '/workspace/wisent-guard'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Fix wandb connection issues\n",
    "def fix_wandb_connection():\n",
    "    \"\"\"Fix wandb connection issues by properly initializing or disabling it.\"\"\"\n",
    "    try:\n",
    "        import wandb\n",
    "        \n",
    "        # Check if wandb is already initialized\n",
    "        if wandb.run is not None:\n",
    "            print(\"‚ö†Ô∏è Cleaning up existing wandb run...\")\n",
    "            wandb.finish()\n",
    "        \n",
    "        # Clear any broken connections\n",
    "        import subprocess\n",
    "        import signal\n",
    "        try:\n",
    "            # Kill any hanging wandb processes\n",
    "            subprocess.run(['pkill', '-f', 'wandb'], capture_output=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"‚úÖ Wandb connection cleaned up\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Wandb cleanup warning: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check HuggingFace authentication\n",
    "def check_hf_auth():\n",
    "    \"\"\"Check if user is logged into HuggingFace and show login instructions if needed.\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['huggingface-cli', 'whoami'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            username = result.stdout.strip()\n",
    "            print(f\"‚úÖ Logged into HuggingFace as: {username}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Not logged into HuggingFace!\")\n",
    "            print(\"üîê Please run: huggingface-cli login\")\n",
    "            print(\"   This is required to access datasets like AIME 2024/2025\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not check HuggingFace authentication: {e}\")\n",
    "        print(\"üîê If you encounter dataset loading issues, try: huggingface-cli login\")\n",
    "        return False\n",
    "\n",
    "# Clean up wandb first\n",
    "wandb_ok = fix_wandb_connection()\n",
    "\n",
    "# Import comprehensive evaluation framework\n",
    "from wisent_guard.core.evaluation.comprehensive import (\n",
    "    ComprehensiveEvaluationConfig,\n",
    "    ComprehensiveEvaluationPipeline,\n",
    "    plot_evaluation_results,\n",
    "    create_results_dashboard,\n",
    "    generate_summary_report,\n",
    "    calculate_comprehensive_metrics,\n",
    "    generate_performance_summary\n",
    ")\n",
    "\n",
    "# Visualization and interactivity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üíæ HuggingFace cache: {os.environ['HF_HOME']}\")\n",
    "print(f\"üîó Wandb status: {'‚úÖ Ready' if wandb_ok else '‚ö†Ô∏è May have issues'}\")\n",
    "print()\n",
    "\n",
    "# Check authentication\n",
    "hf_authenticated = check_hf_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Edit the constants in the next cell to customize your evaluation. All parameters are clearly documented with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Available math tasks: ['aime', 'aime2024', 'aime2025', 'gsm8k', 'hendrycks_math', 'hmmt', 'hmmt_feb_2025', 'livemathbench', 'livemathbench_cnmo_en', 'livemathbench_cnmo_zh', 'math', 'math500', 'polymath', 'polymath_en_high', 'polymath_en_medium', 'polymath_zh_high', 'polymath_zh_medium']\n",
      "üìã CONFIGURATION SUMMARY\n",
      "==================================================\n",
      "ü§ñ Model: /workspace/models/llama31-8b-instruct-hf\n",
      "üìä Datasets: gsm8k ‚Üí gsm8k ‚Üí gsm8k\n",
      "üî¢ Samples: 5 + 5 + 30 = 40 total\n",
      "üéØ Probe layers: [15]\n",
      "‚öôÔ∏è Steering layers: [15]\n",
      "üéõÔ∏è Steering method: DAC (Dynamic Activation Composition)\n",
      "üìä DAC Hyperparameters:\n",
      "   ‚Ä¢ Entropy thresholds: [1.0]\n",
      "   ‚Ä¢ Ptop values: [0.5]\n",
      "   ‚Ä¢ Max alpha values: [2.0]\n",
      "üìö Using tasks from MATH_TASKS: ‚úì\n",
      "üß™ Total hyperparameter combinations: 3\n",
      "üìà Wandb enabled: False\n",
      "üèóÔ∏è Model layers: 32\n",
      "\n",
      "‚úÖ Configuration validated successfully!\n",
      "üí° This evaluation now uses REAL mathematical training data instead of synthetic generation!\n",
      "üéØ DAC will be trained on actual math questions from your training dataset.\n",
      "üí° Configuration updated for quick testing with distilgpt2 and GSM8K dataset.\n",
      "üéØ Currently configured for: gsm8k\n",
      "üìö Available math tasks: 17 tasks including GSM8K, MATH-500, AIME, etc.\n",
      "üí° To change datasets, edit TRAIN_DATASET, VAL_DATASET, TEST_DATASET above.\n"
     ]
    }
   ],
   "source": [
    "# Configuration Constants - Edit these values to customize your evaluation\n",
    "\n",
    "# Import math tasks from our task configuration\n",
    "import sys\n",
    "sys.path.append('/workspace/wisent-guard')\n",
    "from wisent_guard.parameters.task_config import MATH_TASKS\n",
    "\n",
    "# Import DAC steering method\n",
    "from wisent_guard.core.steering_methods.dac import DAC\n",
    "\n",
    "# Convert MATH_TASKS set to sorted list for easier selection\n",
    "MATH_TASKS_LIST = sorted(list(MATH_TASKS))\n",
    "print(f\"üìö Available math tasks: {MATH_TASKS_LIST}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN CONFIGURATION - Edit these constants to customize your evaluation\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'distilbert/distilgpt2'  # Using smaller model for quick testing - Examples: 'distilbert/distilgpt2', 'gpt2', '/workspace/models/llama31-8b-instruct-hf', 'Qwen/Qwen3-8B'\n",
    "MODEL_NAME = \"/workspace/models/llama31-8b-instruct-hf\"\n",
    "\n",
    "# Dataset configuration - Choose from MATH_TASKS\n",
    "TRAIN_DATASET = 'gsm8k'     # Training dataset - Change to any task from MATH_TASKS_LIST\n",
    "VAL_DATASET = 'gsm8k'       # Validation dataset - Change to any task from MATH_TASKS_LIST  \n",
    "TEST_DATASET = 'gsm8k'      # Test dataset - Change to any task from MATH_TASKS_LIST\n",
    "\n",
    "# Validate dataset choices\n",
    "for dataset, name in [(TRAIN_DATASET, 'TRAIN'), (VAL_DATASET, 'VAL'), (TEST_DATASET, 'TEST')]:\n",
    "    if dataset not in MATH_TASKS:\n",
    "        raise ValueError(f\"{name}_DATASET '{dataset}' not in MATH_TASKS. Choose from: {MATH_TASKS_LIST}\")\n",
    "\n",
    "# Sample limits (small for quick testing)\n",
    "TRAIN_LIMIT = 5   # Number of training samples\n",
    "VAL_LIMIT = 5    # Number of validation samples  \n",
    "TEST_LIMIT = 30     # Number of test samples\n",
    "\n",
    "# Layer configuration - specify which layers to search during optimization\n",
    "PROBE_LAYERS = [3]     # Examples: [2, 3, 4, 5], [8, 16, 24, 32], [5, 6, 7, 8] \n",
    "STEERING_LAYERS = [3]  # Same as probe layers for now - Examples: [3, 4, 5], [16, 24, 32], [6, 8, 10]\n",
    "\n",
    "PROBE_LAYERS = [15]     # Examples: [2, 3, 4, 5], [8, 16, 24, 32], [5, 6, 7, 8] \n",
    "STEERING_LAYERS = [15]  # Same as probe layers for now - Examples: [3, 4, 5], [16, 24, 32], [6, 8, 10]\n",
    "\n",
    "\n",
    "# DAC Hyperparameters - specify arrays of values to search\n",
    "ENTROPY_THRESHOLDS = [1.0]    # Examples: [0.5, 1.0, 1.5], [1.0, 2.0], [0.8, 1.2]\n",
    "PTOP_VALUES = [0.5]            # Examples: [0.3, 0.4, 0.5], [0.4], [0.2, 0.6]  \n",
    "MAX_ALPHA_VALUES = [2.0]       # Examples: [1.5, 2.0, 2.5], [2.0], [1.0, 3.0]\n",
    "\n",
    "# Options\n",
    "ENABLE_WANDB = False                        # Disable for quick testing\n",
    "EXPERIMENT_NAME = 'dac_hyperparameter_search'  # Experiment name for logging\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET SIZE MAPPING (for validation - don't edit unless adding new datasets)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-VALIDATION AND INFO\n",
    "# ============================================================================\n",
    "\n",
    "def detect_model_layers(model_name):\n",
    "    \"\"\"Detect number of layers in a model without loading it fully.\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoConfig\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        # Different models store layer count differently\n",
    "        if hasattr(config, 'n_layer'):\n",
    "            return config.n_layer\n",
    "        elif hasattr(config, 'num_hidden_layers'):\n",
    "            return config.num_hidden_layers\n",
    "        elif hasattr(config, 'num_layers'):\n",
    "            return config.num_layers\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Validate configuration\n",
    "print(\"üìã CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìä Datasets: {TRAIN_DATASET} ‚Üí {VAL_DATASET} ‚Üí {TEST_DATASET}\")\n",
    "print(f\"üî¢ Samples: {TRAIN_LIMIT} + {VAL_LIMIT} + {TEST_LIMIT} = {TRAIN_LIMIT + VAL_LIMIT + TEST_LIMIT} total\")\n",
    "print(f\"üéØ Probe layers: {PROBE_LAYERS}\")\n",
    "print(f\"‚öôÔ∏è Steering layers: {STEERING_LAYERS}\")\n",
    "print(f\"üéõÔ∏è Steering method: DAC (Dynamic Activation Composition)\")\n",
    "print(f\"üìä DAC Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ Entropy thresholds: {ENTROPY_THRESHOLDS}\")  \n",
    "print(f\"   ‚Ä¢ Ptop values: {PTOP_VALUES}\")\n",
    "print(f\"   ‚Ä¢ Max alpha values: {MAX_ALPHA_VALUES}\")\n",
    "print(f\"üìö Using tasks from MATH_TASKS: ‚úì\")\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = (len(STEERING_LAYERS) * \n",
    "                     len(ENTROPY_THRESHOLDS) *\n",
    "                     len(PTOP_VALUES) *\n",
    "                     len(MAX_ALPHA_VALUES) *\n",
    "                     len(PROBE_LAYERS) * \n",
    "                     3)  # Assuming 3 probe C values\n",
    "\n",
    "print(f\"üß™ Total hyperparameter combinations: {total_combinations}\")\n",
    "print(f\"üìà Wandb enabled: {ENABLE_WANDB}\")\n",
    "\n",
    "# Model info\n",
    "try:\n",
    "    num_layers = detect_model_layers(MODEL_NAME)\n",
    "    print(f\"üèóÔ∏è Model layers: {num_layers}\")\n",
    "    if isinstance(num_layers, int):\n",
    "        max_probe_layer = max(PROBE_LAYERS) if PROBE_LAYERS else 0\n",
    "        max_steering_layer = max(STEERING_LAYERS) if STEERING_LAYERS else 0\n",
    "        if max_probe_layer >= num_layers or max_steering_layer >= num_layers:\n",
    "            print(\"‚ö†Ô∏è WARNING: Some configured layers exceed model size!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not detect model layers: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration validated successfully!\")\n",
    "print(\"üí° This evaluation now uses REAL mathematical training data instead of synthetic generation!\")\n",
    "print(\"üéØ DAC will be trained on actual math questions from your training dataset.\")\n",
    "print(\"üí° Configuration updated for quick testing with distilgpt2 and GSM8K dataset.\")\n",
    "print(f\"üéØ Currently configured for: {TRAIN_DATASET}\")\n",
    "print(f\"üìö Available math tasks: {len(MATH_TASKS_LIST)} tasks including GSM8K, MATH-500, AIME, etc.\")\n",
    "print(\"üí° To change datasets, edit TRAIN_DATASET, VAL_DATASET, TEST_DATASET above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Create Configuration\n",
    "\n",
    "Configuration is automatically created from the constants defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create configuration from constants\n\nconfig = ComprehensiveEvaluationConfig(\n    model_name=MODEL_NAME,\n    train_dataset=TRAIN_DATASET,\n    val_dataset=VAL_DATASET,\n    test_dataset=TEST_DATASET,\n    train_limit=TRAIN_LIMIT,\n    val_limit=VAL_LIMIT,\n    test_limit=TEST_LIMIT,\n    probe_layers=PROBE_LAYERS,\n    steering_layers=STEERING_LAYERS,\n    steering_methods=[\"dac\"],  # Fixed to DAC only\n    # DAC hyperparameters\n    dac_entropy_thresholds=ENTROPY_THRESHOLDS,\n    dac_ptop_values=PTOP_VALUES,\n    dac_max_alpha_values=MAX_ALPHA_VALUES,\n    enable_wandb=ENABLE_WANDB,\n    experiment_name=EXPERIMENT_NAME,\n    batch_size=16,\n    max_length=512,\n    max_new_tokens=256  # Increased from default 50 for GSM8K chain-of-thought\n)\n\nprint(\"‚úÖ Configuration object created successfully!\")\nprint(\"üöÄ Ready to run comprehensive evaluation!\")\nprint(\"\\nüí° All configuration is now controlled by the constants in the previous cell.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Comprehensive Evaluation\n",
    "\n",
    "This is the main evaluation cell. It will:\n",
    "\n",
    "1. **üéØ Train Probes**: Train correctness classifiers on all specified layers\n",
    "2. **‚öôÔ∏è Optimize Hyperparameters**: Grid search for best steering + probe combinations\n",
    "3. **üèÜ Final Evaluation**: Test optimized configuration on held-out test set\n",
    "\n",
    "**Note**: This may take several minutes depending on your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ STARTING COMPREHENSIVE EVALUATION WITH REAL MATHEMATICAL DATA\n",
      "================================================================================\n",
      "‚úÖ DAC will be trained on actual GSM8K mathematical questions from training data\n",
      "üéØ Using task extractors for proper format handling across datasets\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef991d48a73648afa5e503ef811e3ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING COMPREHENSIVE EVALUATION WITH REAL MATHEMATICAL DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ DAC will be trained on actual {TRAIN_DATASET.upper()} mathematical questions from training data\")\n",
    "print(f\"üéØ Using task extractors for proper format handling across datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ComprehensiveEvaluationPipeline(config)\n",
    "\n",
    "# Run evaluation with progress tracking\n",
    "try:\n",
    "    results = pipeline.run_comprehensive_evaluation()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    evaluation_results = results\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Evaluation failed: {str(e)}\")\n",
    "    print(\"Check the logs above for more details.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Results Analysis\n",
    "\n",
    "Now let's analyze the results with comprehensive metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "comprehensive_metrics = calculate_comprehensive_metrics(evaluation_results)\n",
    "\n",
    "# Generate performance summary\n",
    "performance_summary = generate_performance_summary(comprehensive_metrics)\n",
    "print(performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Interactive Visualizations\n",
    "\n",
    "Explore your results with interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "dashboard = create_results_dashboard(evaluation_results)\n",
    "dashboard.show()\n",
    "\n",
    "print(\"\\nüìä Interactive dashboard displayed above!\")\n",
    "print(\"üí° Hover over points and bars for detailed information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Detailed Analysis: Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract benchmark results\n",
    "if \"test_results\" in evaluation_results:\n",
    "    test_results = evaluation_results[\"test_results\"]\n",
    "    \n",
    "    base_benchmark = test_results.get(\"base_model_benchmark_results\", {})\n",
    "    steered_benchmark = test_results.get(\"steered_model_benchmark_results\", {})\n",
    "    \n",
    "    print(\"üéØ BENCHMARK PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"\\nüìä Base Model:\")\n",
    "    print(f\"  ‚úì Accuracy: {base_benchmark.get('accuracy', 0):.3f} ({base_benchmark.get('accuracy', 0)*100:.1f}%)\")\n",
    "    print(f\"  ‚úì Correct: {base_benchmark.get('correct', 0)}/{base_benchmark.get('total_samples', 0)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Steered Model:\")\n",
    "    print(f\"  ‚úì Accuracy: {steered_benchmark.get('accuracy', 0):.3f} ({steered_benchmark.get('accuracy', 0)*100:.1f}%)\")\n",
    "    print(f\"  ‚úì Correct: {steered_benchmark.get('correct', 0)}/{steered_benchmark.get('total_samples', 0)}\")\n",
    "    \n",
    "    improvement = steered_benchmark.get('accuracy', 0) - base_benchmark.get('accuracy', 0)\n",
    "    improvement_percent = (improvement / max(base_benchmark.get('accuracy', 0.001), 0.001)) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Improvement:\")\n",
    "    print(f\"  {'‚úÖ' if improvement > 0 else '‚ùå'} {improvement:+.3f} absolute ({improvement_percent:+.1f}% relative)\")\n",
    "    \n",
    "    if improvement > 0.05:\n",
    "        print(\"  üéâ Significant improvement! Steering is working well.\")\n",
    "    elif improvement > 0.01:\n",
    "        print(\"  üëç Moderate improvement. Consider tuning hyperparameters.\")\n",
    "    elif improvement > -0.01:\n",
    "        print(\"  ‚ö™ Minimal change. Steering may not be effective for this configuration.\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Performance decreased. Check steering implementation.\")\n",
    "else:\n",
    "    print(\"‚ùå No test results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Detailed Analysis: Probe Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probe results\n",
    "if \"test_results\" in evaluation_results:\n",
    "    base_probe = test_results.get(\"base_model_probe_results\", {})\n",
    "    steered_probe = test_results.get(\"steered_model_probe_results\", {})\n",
    "    \n",
    "    print(\"üîç PROBE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"\\nüìä Base Model Probe:\")\n",
    "    print(f\"  ‚úì AUC: {base_probe.get('auc', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Accuracy: {base_probe.get('accuracy', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Precision: {base_probe.get('precision', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Recall: {base_probe.get('recall', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì F1-Score: {base_probe.get('f1', 0.5):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Steered Model Probe:\")\n",
    "    print(f\"  ‚úì AUC: {steered_probe.get('auc', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Accuracy: {steered_probe.get('accuracy', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Precision: {steered_probe.get('precision', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì Recall: {steered_probe.get('recall', 0.5):.3f}\")\n",
    "    print(f\"  ‚úì F1-Score: {steered_probe.get('f1', 0.5):.3f}\")\n",
    "    \n",
    "    auc_improvement = steered_probe.get('auc', 0.5) - base_probe.get('auc', 0.5)\n",
    "    \n",
    "    print(f\"\\nüìà AUC Improvement:\")\n",
    "    print(f\"  {'‚úÖ' if auc_improvement > 0 else '‚ùå'} {auc_improvement:+.3f}\")\n",
    "    \n",
    "    # Interpret probe performance\n",
    "    best_auc = max(base_probe.get('auc', 0.5), steered_probe.get('auc', 0.5))\n",
    "    \n",
    "    if best_auc > 0.9:\n",
    "        print(\"  üéâ Excellent probe performance! Activations strongly predict correctness.\")\n",
    "    elif best_auc > 0.8:\n",
    "        print(\"  üëç Good probe performance. Activations are informative.\")\n",
    "    elif best_auc > 0.7:\n",
    "        print(\"  ‚ö™ Moderate probe performance. Some signal present.\")\n",
    "    elif best_auc > 0.6:\n",
    "        print(\"  ‚ö†Ô∏è Weak probe performance. Limited interpretability.\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Poor probe performance. Activations may not encode correctness.\")\n",
    "else:\n",
    "    print(\"‚ùå No test results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Hyperparameter Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hyperparameter optimization results\n",
    "if \"steering_optimization_results\" in evaluation_results:\n",
    "    opt_results = evaluation_results[\"steering_optimization_results\"]\n",
    "    all_configs = opt_results.get(\"all_configs\", [])\n",
    "    best_config = opt_results.get(\"best_config\", {})\n",
    "    \n",
    "    print(\"‚öôÔ∏è HYPERPARAMETER OPTIMIZATION ANALYSIS\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    print(f\"\\nüìä Search Statistics:\")\n",
    "    print(f\"  ‚úì Configurations tested: {len(all_configs)}\")\n",
    "    print(f\"  ‚úì Best combined score: {opt_results.get('best_combined_score', 0):.3f}\")\n",
    "    \n",
    "    if best_config:\n",
    "        steering_config = best_config.get(\"steering_config\", {})\n",
    "        probe_config = best_config.get(\"best_probe_config\", {})\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Configuration:\")\n",
    "        print(f\"  ‚úì Steering method: {steering_config.get('method', 'N/A')}\")\n",
    "        print(f\"  ‚úì Steering layer: {steering_config.get('layer', 'N/A')}\")\n",
    "        print(f\"  ‚úì Steering strength: {steering_config.get('strength', 'N/A')}\")\n",
    "        print(f\"  ‚úì Probe layer: {probe_config.get('layer', 'N/A')}\")\n",
    "        print(f\"  ‚úì Probe C value: {probe_config.get('C', 'N/A')}\")\n",
    "        \n",
    "        benchmark_metrics = best_config.get(\"benchmark_metrics\", {})\n",
    "        probe_metrics = best_config.get(\"probe_metrics\", {})\n",
    "        \n",
    "        print(f\"\\nüìà Best Configuration Performance:\")\n",
    "        print(f\"  ‚úì Benchmark accuracy: {benchmark_metrics.get('accuracy', 0):.3f}\")\n",
    "        print(f\"  ‚úì Probe AUC: {probe_metrics.get('auc', 0.5):.3f}\")\n",
    "        print(f\"  ‚úì Combined score: {best_config.get('combined_score', 0):.3f}\")\n",
    "    \n",
    "    # Analyze score distribution\n",
    "    if all_configs:\n",
    "        scores = [config.get(\"combined_score\", 0) for config in all_configs]\n",
    "        benchmark_scores = [config.get(\"benchmark_metrics\", {}).get(\"accuracy\", 0) for config in all_configs]\n",
    "        probe_scores = [config.get(\"probe_metrics\", {}).get(\"auc\", 0.5) for config in all_configs]\n",
    "        \n",
    "        print(f\"\\nüìä Score Distribution:\")\n",
    "        print(f\"  ‚úì Combined score: {np.mean(scores):.3f} ¬± {np.std(scores):.3f}\")\n",
    "        print(f\"  ‚úì Benchmark score: {np.mean(benchmark_scores):.3f} ¬± {np.std(benchmark_scores):.3f}\")\n",
    "        print(f\"  ‚úì Probe score: {np.mean(probe_scores):.3f} ¬± {np.std(probe_scores):.3f}\")\n",
    "        \n",
    "        # Check if optimization was effective\n",
    "        score_range = max(scores) - min(scores)\n",
    "        if score_range > 0.1:\n",
    "            print(\"  üéØ Good optimization! Significant variation in scores.\")\n",
    "        elif score_range > 0.05:\n",
    "            print(\"  üëç Moderate optimization. Some configurations better than others.\")\n",
    "        else:\n",
    "            print(\"  ‚ö™ Limited optimization benefit. Most configurations perform similarly.\")\n",
    "else:\n",
    "    print(\"‚ùå No optimization results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probe training performance by layer\n",
    "if \"probe_training_results\" in evaluation_results:\n",
    "    training_results = evaluation_results[\"probe_training_results\"]\n",
    "    \n",
    "    print(\"üìä PROBE TRAINING ANALYSIS\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Create summary table\n",
    "    training_data = []\n",
    "    \n",
    "    for layer_key, layer_results in training_results.items():\n",
    "        layer_num = int(layer_key.split('_')[1])\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_config = None\n",
    "        \n",
    "        for c_key, metrics in layer_results.items():\n",
    "            if isinstance(metrics, dict) and \"auc\" in metrics:\n",
    "                if metrics[\"auc\"] > best_auc:\n",
    "                    best_auc = metrics[\"auc\"]\n",
    "                    best_config = c_key\n",
    "        \n",
    "        training_data.append({\n",
    "            'Layer': layer_num,\n",
    "            'Best AUC': best_auc,\n",
    "            'Best C': best_config.replace('C_', '') if best_config else 'N/A'\n",
    "        })\n",
    "    \n",
    "    # Display as formatted table\n",
    "    df_training = pd.DataFrame(training_data).sort_values('Layer')\n",
    "    \n",
    "    print(f\"\\n{'Layer':<8} {'Best AUC':<10} {'Best C':<10}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for _, row in df_training.iterrows():\n",
    "        print(f\"{row['Layer']:<8} {row['Best AUC']:<10.3f} {row['Best C']:<10}\")\n",
    "    \n",
    "    # Find best performing layer\n",
    "    best_layer_row = df_training.loc[df_training['Best AUC'].idxmax()]\n",
    "    worst_layer_row = df_training.loc[df_training['Best AUC'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best performing layer: {best_layer_row['Layer']} (AUC: {best_layer_row['Best AUC']:.3f})\")\n",
    "    print(f\"‚ö†Ô∏è Worst performing layer: {worst_layer_row['Layer']} (AUC: {worst_layer_row['Best AUC']:.3f})\")\n",
    "    \n",
    "    # Layer performance insights\n",
    "    auc_std = df_training['Best AUC'].std()\n",
    "    if auc_std > 0.1:\n",
    "        print(\"\\nüí° High variation across layers - layer choice matters!\")\n",
    "    elif auc_std > 0.05:\n",
    "        print(\"\\nüí° Moderate variation - some layers work better than others.\")\n",
    "    else:\n",
    "        print(\"\\nüí° Consistent performance across layers.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No training results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Static Visualizations\n",
    "\n",
    "Create comprehensive static plots for reports and publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive static visualization\n",
    "fig = plot_evaluation_results(evaluation_results)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Comprehensive evaluation plots displayed above.\")\n",
    "print(\"üíæ Plots are saved automatically in the results directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Results\n",
    "\n",
    "Save your results in various formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Export and Storage Options\n",
    "print(\"üìä RESULTS STORAGE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if wandb is enabled and results were logged\n",
    "if config.enable_wandb:\n",
    "    print(\"‚úÖ Weights & Biases logging is ENABLED\")\n",
    "    print(\"üìà All evaluation results have been automatically logged to wandb including:\")\n",
    "    print(\"   ‚Ä¢ Configuration parameters\")\n",
    "    print(\"   ‚Ä¢ Probe training metrics\")\n",
    "    print(\"   ‚Ä¢ Hyperparameter optimization results\") \n",
    "    print(\"   ‚Ä¢ Final test performance\")\n",
    "    print(\"   ‚Ä¢ Comprehensive metrics and visualizations\")\n",
    "    print()\n",
    "    print(\"üîó Access your results on the wandb dashboard:\")\n",
    "    print(\"   https://wandb.ai/\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Weights & Biases logging is DISABLED\")\n",
    "    print(\"üíæ Creating local backup files...\")\n",
    "    \n",
    "    # Create results directory in outputs/ (excluded from git)\n",
    "    results_dir = Path(\"outputs/notebook_results\")\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Save raw results as JSON\n",
    "    json_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        # Remove non-serializable objects\n",
    "        import copy\n",
    "        results_copy = copy.deepcopy(evaluation_results)\n",
    "        \n",
    "        def remove_non_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: remove_non_serializable(v) for k, v in obj.items() if k != 'probe'}\n",
    "            elif isinstance(obj, list):\n",
    "                return [remove_non_serializable(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        clean_results = remove_non_serializable(results_copy)\n",
    "        json.dump(clean_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Raw results saved to: {json_file}\")\n",
    "    \n",
    "    # 2. Save comprehensive metrics as CSV\n",
    "    csv_file = results_dir / f\"comprehensive_metrics_{timestamp}.csv\"\n",
    "    metrics_df = pd.DataFrame([comprehensive_metrics])\n",
    "    metrics_df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Comprehensive metrics saved to: {csv_file}\")\n",
    "    \n",
    "    # 3. Generate HTML report\n",
    "    html_report = generate_summary_report(evaluation_results, config.to_dict())\n",
    "    html_file = results_dir / f\"evaluation_report_{timestamp}.html\"\n",
    "    with open(html_file, 'w') as f:\n",
    "        f.write(html_report)\n",
    "    \n",
    "    print(f\"‚úÖ HTML report saved to: {html_file}\")\n",
    "    \n",
    "    # 4. Save configuration\n",
    "    config_file = results_dir / f\"configuration_{timestamp}.json\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config.to_dict(), f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Configuration saved to: {config_file}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All results saved in: {results_dir.absolute()}\")\n",
    "\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "if config.enable_wandb:\n",
    "    print(\"   Use wandb dashboard for comprehensive result analysis and comparison.\")\n",
    "    print(\"   Results are automatically versioned and shareable via wandb.\")\n",
    "else:\n",
    "    print(\"   Enable wandb logging for better experiment tracking and result management.\")\n",
    "    print(\"   Set enable_wandb=True in the configuration for future runs.\")\n",
    "\n",
    "print(\"\\nüéâ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Optional: Detailed Data Exploration\n",
    "\n",
    "Use this section to explore specific aspects of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive data exploration\n",
    "@interact\n",
    "def explore_results(\n",
    "    section=widgets.Dropdown(\n",
    "        options=['Configuration', 'Training Results', 'Optimization Results', 'Test Results'],\n",
    "        value='Configuration'\n",
    "    )\n",
    "):\n",
    "    if section == 'Configuration':\n",
    "        display(Markdown(\"### üìã Configuration Details\"))\n",
    "        # Fix UnboundLocalError by accessing config from global scope\n",
    "        if 'config' in globals():\n",
    "            config_df = pd.DataFrame(list(config.to_dict().items()), columns=['Parameter', 'Value'])\n",
    "            display(config_df)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Configuration not available. Please run the configuration cell first.\")\n",
    "        \n",
    "    elif section == 'Training Results':\n",
    "        display(Markdown(\"### üéØ Probe Training Results\"))\n",
    "        if \"probe_training_results\" in evaluation_results:\n",
    "            training_data = []\n",
    "            for layer_key, layer_results in evaluation_results[\"probe_training_results\"].items():\n",
    "                layer_num = int(layer_key.split('_')[1])\n",
    "                for c_key, metrics in layer_results.items():\n",
    "                    if isinstance(metrics, dict) and \"auc\" in metrics:\n",
    "                        training_data.append({\n",
    "                            'Layer': layer_num,\n",
    "                            'C': float(c_key.replace('C_', '')),\n",
    "                            'Accuracy': metrics.get('accuracy', 0),\n",
    "                            'AUC': metrics.get('auc', 0.5),\n",
    "                            'Precision': metrics.get('precision', 0),\n",
    "                            'Recall': metrics.get('recall', 0),\n",
    "                            'F1': metrics.get('f1', 0)\n",
    "                        })\n",
    "            if training_data:\n",
    "                training_df = pd.DataFrame(training_data)\n",
    "                display(training_df.round(3))\n",
    "        else:\n",
    "            print(\"No training results available.\")\n",
    "            \n",
    "    elif section == 'Optimization Results':\n",
    "        display(Markdown(\"### ‚öôÔ∏è Hyperparameter Optimization Results\"))\n",
    "        if \"steering_optimization_results\" in evaluation_results:\n",
    "            opt_results = evaluation_results[\"steering_optimization_results\"]\n",
    "            all_configs = opt_results.get(\"all_configs\", [])\n",
    "            \n",
    "            if all_configs:\n",
    "                opt_data = []\n",
    "                for i, config_item in enumerate(all_configs):\n",
    "                    steering_config = config_item.get(\"steering_config\", {})\n",
    "                    probe_config = config_item.get(\"best_probe_config\", {})\n",
    "                    \n",
    "                    opt_data.append({\n",
    "                        'Config': i + 1,\n",
    "                        'Steering Method': steering_config.get('method', 'N/A'),\n",
    "                        'Steering Layer': steering_config.get('layer', 'N/A'),\n",
    "                        'Steering Strength': steering_config.get('strength', 'N/A'),\n",
    "                        'Probe Layer': probe_config.get('layer', 'N/A'),\n",
    "                        'Probe C': probe_config.get('C', 'N/A'),\n",
    "                        'Benchmark Accuracy': config_item.get('benchmark_metrics', {}).get('accuracy', 0),\n",
    "                        'Probe AUC': config_item.get('probe_metrics', {}).get('auc', 0.5),\n",
    "                        'Combined Score': config_item.get('combined_score', 0)\n",
    "                    })\n",
    "                \n",
    "                opt_df = pd.DataFrame(opt_data)\n",
    "                display(opt_df.round(3))\n",
    "        else:\n",
    "            print(\"No optimization results available.\")\n",
    "            \n",
    "    elif section == 'Test Results':\n",
    "        display(Markdown(\"### üèÜ Final Test Results\"))\n",
    "        if \"test_results\" in evaluation_results:\n",
    "            test_results = evaluation_results[\"test_results\"]\n",
    "            \n",
    "            # Create summary table\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Base Benchmark Accuracy',\n",
    "                    'Steered Benchmark Accuracy', \n",
    "                    'Base Probe AUC',\n",
    "                    'Steered Probe AUC',\n",
    "                    'Validation Combined Score'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    test_results.get('base_model_benchmark_results', {}).get('accuracy', 0),\n",
    "                    test_results.get('steered_model_benchmark_results', {}).get('accuracy', 0),\n",
    "                    test_results.get('base_model_probe_results', {}).get('auc', 0.5),\n",
    "                    test_results.get('steered_model_probe_results', {}).get('auc', 0.5),\n",
    "                    test_results.get('validation_combined_score', 0)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            display(summary_df.round(3))\n",
    "        else:\n",
    "            print(\"No test results available.\")\n",
    "\n",
    "print(\"üîç Use the dropdown above to explore different sections of your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Congratulations! You've successfully run a comprehensive evaluation that separates:\n",
    "\n",
    "1. **üéØ Benchmark Performance**: How well your model solves problems\n",
    "2. **üîç Probe Performance**: How well we can detect when the model is wrong\n",
    "3. **‚öôÔ∏è Optimization**: Finding the best configurations through proper validation\n",
    "\n",
    "### Next Steps:\n",
    "- üìä Analyze the results above to understand your model's behavior\n",
    "- üîß Try different configurations to see how they affect performance\n",
    "- üìà Use the exported results for further analysis or reporting\n",
    "- üöÄ Scale up to larger models and datasets when ready\n",
    "\n",
    "### Key Insights:\n",
    "- The framework properly separates model capability from interpretability\n",
    "- Hyperparameter optimization validates on actual performance, not just probe metrics\n",
    "- Results are saved and visualized for easy interpretation\n",
    "\n",
    "Happy experimenting! üß™‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}